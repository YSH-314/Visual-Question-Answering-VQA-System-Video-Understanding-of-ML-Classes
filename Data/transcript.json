[
    {
        "transcript": "As you guys have probably already noticed that I'm currently umm assistant professor at our AI program.",
        "week": 1,
        "page": 1
    },
    {
        "transcript": "And let me quickly talk about my research. So, here is my email address, if you have any questions regarding to your course or regarding to your future, you know, or any other minor stuff you can feel free send me an email. My research and regular focus is on these 4 directions. So first of all, I'll majority of my papers, I focus on transfer learning. So specifically, I focused on as more domain for the domain adaptation. So later I will explain what the domain adaptation meaning here. Next, I am interested in manifold learning, which is kind of an expansion of space is different from our training space. And next, I am interested in the shape analysis, and this is really, relatively related to the medical field. And lastly, recently, I'm more focused on the audio denoising problem.",
        "week": 1,
        "page": 2
    },
    {
        "transcript": "Let's see some of examples. So, first, right is about the domain adaptation. So, in this slide, we will see our source domain and the test domain. And during in our Machine Learning, we, we, we have like the training data set and test data. But in this case, we will refer our training data set as a source domain and our test data set as a target domain. So now as you can see there, you cannot see significant difference between your source domain and the target domain. So here, for example, those last 3 objects from the product domain, and here in just about the outline of the clipart domain. So now you will observe the difference between these 2 domains.  If you apply some existing Machine Learning models, you may not get a high accuracy. And those traditional machine models where include all models that I will cover in the fall semester. Which means that for those traditional methods, it will not get a very high accuracy. And similarly, we have another kind of data set. In the medical field, for example, we have the T1 images, and we have the flair images. If we try to segment the T1 images focused on the parameter, if we are then applying our model to the flair image, we will and not get a very good result because you see the difference between your tumor images and the flair images, and even worse, your source domain can be only like images, but your target domain can be just some text. Now you can direct the difference between images and the task. Right? And then this is the problem of the domain adaptation that we want to mitigate. The difference between a training and a test domain.",
        "week": 1,
        "page": 3
    },
    {
        "transcript": "And I will introduce one more of the previous that I developed out of these examples. That's exactly how manifold, which means that if we are keeping sampling some samples, from the source domain to the target domain. So now, you can see this image in the shoes. How monkey changes the interviewer working human being right. It's corresponding to those plan dot here. So, this is about the time frame of the many goals.",
        "week": 1,
        "page": 4
    },
    {
        "transcript": "And also, this is about another application in the medical field that causes shape analysis, which means that we want to check your shift changes with the increasing of your age. As you probably noticed that with the increasing of age, you may more likely to get some diseases. Right? And this is the meaning of this task. So, for example, your the input of all the network is just the age. For example, in this case, it's just 60. And the output is the output is just about one the shape of your brain structure. So here, it's specifically referred to the corpus callosum, which is one of our brain structures that is related to your memory. And, specifically, we are related to the outcome of diseases. If you found that your corpus callosum is very small, you are more likely to get autonomic diseases. Right? So then get a prediction model. You cannot use both to prevent getting those kinds of diseases.",
        "week": 1,
        "page": 5
    },
    {
        "transcript": "And we also have applications. We rely see the he's the animals like here in just about fly heart segmentation. The purpose of here is to try to segment the fly heart area. And then you can see how this area keeps changes. Right?",
        "week": 1,
        "page": 6
    },
    {
        "transcript": "And we also have other applications in the cow teat segmentation and classification. So again, I just use some results about the cow teat area. Those keep in the bounding box to refer to the location, and at this given color segment a teat area. And this different number means different classes of the diseases. If it is 1, it means normal. If it is 2, it means it's abnormal. So sometimes, you will see there two there. Because the truth is that most of the cows are very healthy. Right? Just a few of them can get diseases. But then later, you will see a lot of them.",
        "week": 1,
        "page": 7
    },
    {
        "transcript": "And, basically, we have another lecture about audio denoising. You probably have noticed that audio denoising is more likely in this another field that's not, for example, in the internal process. However, we also converted the audio signals to audio images. So, after that, we can do some kind of process of the audios. But eventually upload and get some segmentation model. We can try to can segment the audio images, after we apply the inverse of short time Fourier transformation. We can get a denoised audio.",
        "week": 1,
        "page": 8
    },
    {
        "transcript": "And now we can quickly listen. For example, in this case. Can you hear anything? But if you look at the Signal here in the left hand, you see a lot of, like, noise, you just bring it over. But On your right hand, once you get the denoised, you can see a clear pattern of this sound. Right? This is called a realization of the purpose of audio denoising.",
        "week": 1,
        "page": 9
    },
    {
        "transcript": "And then even further, we have other applications related to the audio separation. So, you can also pay attention to these 2 figures, as you can see here. Right? See, on the left hand, this figure is about the noisy mixing birds. By the right hand, there are 2 different colors. So, the yellow color means the bird sound 1, here and the blue color means the bird sound 2. So, this is, um, becomes, becomes an image segmentation problem. Right? ",
        "week": 1,
        "page": 10
    },
    {
        "transcript": "Also, we have other applications that are about audio enhancements. As you can see in the left and also in the blue color refers to the noise bird sound and this little bit of this red area refers to the bird sound as you can see from here. The original audio is very tiny, very small. Right? However, if you apply the audio enhancement you can see it very big. It's very annoying that you cannot hear the audio enhanced sounds. I'm trying to play it. This is actually the denoised bird sound, very tiny. This one is pretty big.",
        "week": 1,
        "page": 11
    },
    {
        "transcript": "Now always, for our online students, if you have any questions, basically, approach to stop me. I would like to hear the feedback from you guys. Okay? Do not be shy. Okay. Also, any questions? So now, probably we are starting with our Machine Learning course. So that's the first time with the course overview. And some things that is very important to all of you, specifically the final score. Right?",
        "week": 1,
        "page": 12
    },
    {
        "transcript": "So here is the textbook, you guys might need to read. It is called like a bible in Machine Learning field. In it is like written by the Bishop, the Patent Recognition and Machine Learning. And here is an online link, you can click it and download it that is an online free PDF. It is not needed to buy the new one, but you can buy a new one if you want to read the text. And then this here is just our Canvas page, and you can feel free to have some discussions online.",
        "week": 1,
        "page": 13
    },
    {
        "transcript": "And again, about our prospect, I think, all of you should have noticed about that. Right? Actually, the last latter class is May 10th. And our first time is Wednesday, this time, at this location and again the zoom link of the class. For, this is the only very good for the online students. But for others, I very encourage you guys to come in person. Unless you have some good reasons, yes, a good reason. And don't say you are too tired. Okay.",
        "week": 1,
        "page": 14
    },
    {
        "transcript": "And we also have office hours. It, it is in every Wednesday at 4 - 5 PM. And just come to my office. I think most of you, you're all of them come to the sixth floor. It's at 205. I see some of you guys, but some of you, I did not speak on this matter and then see you guys to come. Feel free to stop in the office hour after any kind of questions. Okay?",
        "week": 1,
        "page": 15
    },
    {
        "transcript": "And then again, syllabus, how many of you check out about our syllabus? None of you? Okay. That's fine. I will also cover about that part.",
        "week": 1,
        "page": 16
    },
    {
        "transcript": "So here, I found some prerequisites. So, basically just fundamental mathematics. You already probably get from the first semester. And also, I will even next week, I will try to give you guys a quick review of some math that is required in this class. And I think we've got Python by professor Pablo. It's good.",
        "week": 1,
        "page": 17
    },
    {
        "transcript": "So, here, about assignments, some percentage that you have to know. It's, like, its discussions, some participation is, like, 10 percentage. Homework. And you guys, we have a kind of lots of homework, like each week, we will have some homework. It will not be that difficult. You need to devote some time on it. I'm sure you guys will get it. But finally, I will only select the highest selection of them for your final grade. And lastly, it's about the final project. It's about 30 percentage. For that reason, your final project is like your first year come to my office, talk to me to select your favorite project, you want to work on, so then you need to write a quick proposal for that. At the last day, you need to present your final project, then do and maybe answer questions for your audience. I will not have any exams. But if you guys like exams, tell me, I will make an exam for you. Okay. I, I, I really hope some students tell me you guys want some exam. I can remove this grading system percentage lower if you guys can tell me some exams or final exam. Are you ready? Complete? No. No. Nobody likes exams. Okay. So, let's have to just keep this format. Professor: What about? Student: I have, I have one question about team members. Is there any limits of the numbers of members in a group? Professor: The team member you mean? Student: For the project or the final presentation? Professor: Yes. Yeah. So that's we are depends on your guys. If you guys sound like other at the most, I would like to say 2 students per group. Because, totally, we have around 12 students. Right? So, at the most, like, 2 person in 1 group, but you can do it by yourself. I think in that case, you will get some kind of extra points for that if you're doing it by yourself. Okay. Student: Got it. Thank you. Professor: Any other questions? No. Okay. Great.",
        "week": 1,
        "page": 18
    },
    {
        "transcript": "And I think any of you are from the first semester. No? It's from the second semester. You should know about this in different data grades. Right?",
        "week": 1,
        "page": 19
    },
    {
        "transcript": "And again, about the details of our class schedule. So, we are in the first week, and then next week, we'll talk about basic math, and the data preprocessing. In the third week, we are talking about classification, after that, we will talk about some regression, and least squares, and then we will talk about the PCA. I think there's some of you probably already talked about some, something about the PCA. Right? Okay. Those are the few guys. After, we're gonna talk about the matrix factorization, and the Gaussian mixture model, basically, some neural networks, decision trees or a vector, and we have 2 weeks for holidays. Did you guys notice that? If not that I wanna tell you, you have 2 weeks at a holiday. It'll be a better time to start. And after that, we have, like, neural network, ensemble method, and then we are finished about the whole semester. I have to emphasize that. So, all, like, all the contents in this Machine Learning course are very important to you. Because of your future interviews, they definitely will ask questions from this machine learning class. Remember about that. And even if they will ask you to implement some of these algorithms. Previously, like, one of the algorithms happened to be that is about this decision tree. The interviewer will ask you to implement or reimplement this decision tree from scratch. So, so then, during this homework, you guys are ready to think about the first, like you have an interview. How you can implement those algorithms by yourself. Okay. That is very important. Any questions so far? No?; Student 1: Do we have the final project? Professor: Yes, let's go try to find it into our syllabus again. So, here is the all content that I will cover in the whole semester. As you can see here, we have like final project and some implementation of it. In middle, you only have like thirteen different homeworks. Each of them, you will have a kind of smaller work. Student 1: So, are all the projects independent for that or like a team works? Professor: You mean the homework? Student 1: No. The projects. Professor: The final project, as one student asked, it can be teamwork, but it depends on. It's no more than two persons. Student 2: And for the next interview. Professor: Yeah. For homework, just in the interview, some very small, small work. Like today, we will, I'll ask you guys to answer; What is machine learning and what is the future of it? Maybe next class, I want to give you guys a little bit easy stuff to that. And then for the other middle, I might need you guys to implement it from scratch. So, you treat them carefully, and like during your interview how you will implement those kinds of algorithms. Like Gaussian mixture model, how you will implement. How you will implement the Decision Tree. Right? If you guys are ready, it's a very good job in this class. I mean, in a future view, in a future view, ask you the same questions. We have no question about that. You have immediately implemented. You definitely have a good impression on them. So, they might need might hire you immediately, hopefully. Okay?",
        "week": 1,
        "page": 20
    },
    {
        "transcript": "So now let's talk about passed out today, that is introduction of Machine Learning.",
        "week": 1,
        "page": 21
    },
    {
        "transcript": "Professor: So here in the big picture of Machine Learning, just a particular way we are start from artificial intelligence, that is the name of our program, artificial intelligence. What do you think about artificial intelligence? Student 1: About Artificial Intelligence? It's like human intelligence is imitated by things. Professor: Okay. Good. What about other thoughts? What do you think about Artificial Intelligence? Okay. Good. What about other thoughts? What do you think about Artificial Intelligence? Yes. Exactly. What about other thoughts? This is so. Yeah. It's still kind of. What about other thoughts from online students? Your mind, what do you think about Artificial Intelligence? And did you notice some difference between AI and Machine Learning? Which one is bigger? Yeah. Student 5: If a variety is Machine Learning some of way implementing AI. And since this, AI is a set of methods that can use methods can study by itself to solve some problems. Professor: Yes. Exactly. You got it. In the next few slides, we are first introducing AI and then we are talking about Machine Learning. So, we will talk about AI a little bit. For the other differences between AI and the Machine Learning has already noticed in this bigger. Right?",
        "week": 1,
        "page": 22
    },
    {
        "transcript": "But first of all, for the AI, what is AI? So actually, in one sentence, then more like AI is the science of making intelligent machines which can perform tasks that require intelligence when performed by humans. And therefore, important components that is the 'Perception', the 'Learning', the 'Abstraction', and 'Reasoning'.",
        "week": 1,
        "page": 23
    },
    {
        "transcript": "So, first of all, was the 'Perception' of we also can say it as 'Representation'. So, in AI, 'Perception' is a process to interpret, acquire, select, and then organize the sensory information from the physical world to make actions like humans.",
        "week": 1,
        "page": 24
    },
    {
        "transcript": "But what about 'Learning'? So, the 'Learning' is the ability of a system to improve its behavior based on experience. So, most likely, it's behavior based on experience. Right?",
        "week": 1,
        "page": 25
    },
    {
        "transcript": "But the 'Reasoning'? 'Reasoning' is a way to infer  facts from existing data. It is a general process of thinking rationally, to find valid conclusions. What are the differences between Machine Learning and machine reasoning? Right? So, what is about to find patterns? Machine Learning is always to try to find some logical between these patterns, and why the other one is about to understand relationship, it's more like a human being, with seeing a deep understand. What was the relationship between them? Like tackling new problems with a deductive and inductive reasoning approach.",
        "week": 1,
        "page": 26
    },
    {
        "transcript": "Professor: What about 'Abstraction'? So, 'Abstraction' is a fundamental mechanism underlying both human and artificial perception, representation of knowledge, reasoning and learning. It aims to take knowledge that is discovered at a certain level and applying it at another level. So then, another question. So, among those four different aspects. Which one, but you're assuming it's mostly important or mostly difficult for AI on our desk. So, 'Perception', 'Learning', 'Abstraction' or 'Reasoning'? Which one? Like, which one? What was the challenge? How about you? Student 1: Abstraction. Professor: Why Abstraction? Professor: Yes. You're saying that that is difficult establishes. But yeah. Yeah. That's that is a problem. Right? Student 2: 'Reasoning'. Professor: Ok. What about others? Do you agree with her or you have a different opinion? Okay. So, now, let's direct you to the answer.",
        "week": 1,
        "page": 27
    },
    {
        "transcript": "So, what about the bottleneck of the AI? So, actually, it's just about the abstraction and the reasoning. The key reason is about for the AI is still very difficult for them to make sense of the theory behind all those objects. Especially about reasoning and really about understanding the mechanism behind those problems. Sometimes it is very difficult for, like, human beings. Right? Especially about deductive and inactive. If you want Machine Learning to do some tasks that have never turned before, it is very challenging for them.",
        "week": 1,
        "page": 28
    },
    {
        "transcript": "And here, in a big picture of the AI system architecture. It's more about end-to-end of headlines. There are lots of applications. There's lots of augments there, so probably I would kind start from those 'Robust AI'. There are I only here just mentioned the, like, of 5 different aspects, like the 'Explainable AI', 'Metrics and Bias Assessment', 'Verification & Validation' is more like you like your phone, you use a face to unlock it. Right? And the 'Security', the other is about some 'Policy', 'Ethics', 'Safety', 'Training' problem. And also, there are a lot of modern, modern, like, computing devices as you are seeing on. You definitely guys see CPUs. Right? You know about the GPUs. I've been watching that you already heard about that. And you probably notice about the GPU, which is more robust work than CPU, and you have this 'Custom' and the 'Quantum' computing and other staff. Right? But before, again, those cannot use in Machine Learning models, like that data is always our first task. Right? Altogether, data, here is about 'Data Collection', 'Data Management', like 'Data Labeling'. Especially the amount that the 'Data Labeling' is a big challenge for all existing Machine Learning. For those, you definitely need to provide feedback about a data set. So, in that initial stage, it's a requirement for humans to provide something. So, that is a bit of a reason why 'Data Labeling' is now still very challenging. Also, we have so many different implementing models or Machine Learning models, but 'Data Labeling' is also a very difficult process. And after that, we have some algorithms, yeah, just several different types. Like, 'Unsupervised Learning', 'Supervised Learning', we also have 'Reinforcement Learning', etcetera. Another side, we have, like human and computer interaction so that we can get better performance. So, for the human and the computer interaction, the HCI, and it really would take a little bit longer time than single computer models. But, you know, it'll get a much higher accuracy. So, love it, this area is also very popular. And also, we have several other applications. So, let's mention 1 of all those applications. Any questions? No, okay. Great.",
        "week": 1,
        "page": 29
    },
    {
        "transcript": "Professor: Now, let's talk about what is different between AI and the Machine Learning. So, you already noticed that Machine Learning is just a branch of AI, but what are the details of them? So, for the AI, in that scientific field concerned with the development of algorithms that allow computers to learn without being actually programmed, but for the Machine Learning is just a branch of it which focuses on methods that learn from data and make predictions on unseen data. And then for the Machine Learning, we really care about prediction, channel, that kind of prediction. Right? So, here is the general process of Machine Learning. So, during the training stage, we have a labeled data set. What about label data set? Student 1: Labels are input data. Professor: What do you mean by labeled input data? Student 1: Features and labels. Professor: Features and labels. Yeah. Yes. Exactly. I agree with that. What about other labels in the data set? Oh, you, you questioned that? What is the label of the data? Okay. Great. Yeah. Go ahead. Student 2: A label data? Professor: Yeah. The label data. What is labeled data? Student 2: I think, we can have some datasets, for example, we have several samples, and each sample has some labels that it identifies some class, for example. Professor: Yes, good. I see you guys now appreciate it. Right? So, the label of data will provide some labels for the datasets. For example, like images, give an image of a cat. The label is a cat. You have labeled it. So, it is a labeled data set. Right? So, for example, you've got stock price prediction sold on stock price of your labels of it. Right? Give you give you your time series data. And, yeah, another stage of Machine Learning is called 'prediction' or called the test stage. We have another kind of labeled data set. But we will use this time, we will try to use trained Machine Learning models based on the training stage we can check to make some predictions, for the test data set and then even we want to evaluate the performance the performance of the Machine Learning models. Right? Any question? Okay. Student 1: Is Reinforcement Learning in this category? Professor: I do not have yet that yet. In few slides, but Reinforcement Learning is different from those, so here we will see it. Okay? That's a good question.",
        "week": 1,
        "page": 30
    },
    {
        "transcript": "So next, I want to briefly mention about the history of the Machine Learning.",
        "week": 1,
        "page": 31
    },
    {
        "transcript": "So, definitely, you know, we will go back to a long time ago. Right? It's like 70 years or so ago, but that is in the 1950s, the first light of Machine Learning is very related to the checker player. So out of that, in the 1960s, like the earliest offer of the neural network, it's called Perceptron as it was born at that time. However, after that, it was proven that Perceptron is not working. So then, we'll be able to expand more reasons about their products. In the 1970s, we also have other coming algorithms.",
        "week": 1,
        "page": 32
    },
    {
        "transcript": "So, then, let's go back to the 1980s. So, we have advanced Decision Tree and the Rule Learning after the previous. Later in the 1990s, so, in this case, we will have more bulk view of Machine Learning like that. There's a mark, finding some text and learning, some Reinforcement Learning and Inductive Logic Programming, etcetera.",
        "week": 1,
        "page": 33
    },
    {
        "transcript": "Even recently like in the 2000s. This is like a majority of traditional Machine Learning algorithms that are already invented. At that time, for example, the Support Vector Machine, the kernel methods, the graphical methods, statistical relational learning, from transfer learning, etcetera. And then, it includes you like knowledge about 10 years ago in the 2010s. In that stage and like Deep Learning is ready and becomes popular. Right? So, out of that, in the next decade, like 2010, the Deep Learning is most likely everyday work. So, it going to increase that economy.",
        "week": 1,
        "page": 34
    },
    {
        "transcript": "Next, I would like to introduce what is Machine Learning. And why should we study? How we care about Machine Learning? Right?",
        "week": 1,
        "page": 35
    },
    {
        "transcript": "Have you ever thought? How easily we can recognize face, the color, the shape or handwritten characters. How will the children learn to balance or develop preference to some taste. Right? I don't know whether guys have some time to think about that, or even like human's cognitive abilities have transformed every aspect of our lives. And the human mind is a set of cognitive gadgets, and specialized to learn.",
        "week": 1,
        "page": 36
    },
    {
        "transcript": "So, this is about the intuition of showing how it handles themselves? Actually, there are no specific features that identification given and they just given from the experience. Like an infant, like a poor baby would be content just the first time of experience. Right? Which your parents pose them like we probably noticed that our eyes can take an image like every 200 milliseconds, and then we have like 5 pictures per second, and totally around 300 pictures by per minute, as there are lots of amount of data sets giving as input. But does that mean? Where they're increasing our age, definitely they see more and more images. Right? What about the word? And that's about our children. So really humans learn from experience. Right? Once or sometimes if you make a mistake in that first case, you'll probably, in the next time you try to avoid it. Right? If you, even in the second time, you may make a mistake again, but there's so many times so definitely you can recognize it, and definitely, you can avoid providing some mistakes.",
        "week": 1,
        "page": 37
    },
	{
		"transcript": "And then another good question about how Machine Learning is different from traditional programming? Here's an example here. If I ask you how to write a program like a pseudo code to identify a cat, to identify a cat in an image. How would you do it? Any ideas? Student 1: You mentioned. I'm not very good to do. And I'm here. No ideas, but traditional model is bigger. Professor: Yes. That's a very good spectrum. What about some thoughts about your guys? Use traditional programming, how to identify this cat? Any thoughts? It comes without a result, still without a solution. So, we really talk about some, in the early stage, without computer vision problems. But the kind of the easiest solutions you can think of to how to distinguish, whether this is a cat or not. What about online students? Do you guys have any thoughts? How can you write a program about identifying whether this is a cat? Student 2: For example, if, for example, if some type, like, triangle, and for each for each part of body, like, for example, for eyes, eyes like green or black. And for and for each, for each type of body to make some if statement and check if it's related to the cat or not. Professor: Good. You have a similar solution to your classmate. So, let's see. Student 3: Can I say something, professor? Professor: Do you have another solution? Student 3: Yeah. Yeah. So first, we convert this RGB image to black and white image, and then we make a decision. We can make a classifier like logistic regression classifier or gradient boosting classifier, which classifies the image? Professor: Yeah. So, actually, your answer is similar to Juju. You did use that. This is the more likely some kind of advanced solution.",
		"week": 1,
		"page": 38
	},
	{
		"transcript": "So, let's give us the very, you know, early stage whether we can do for that. As some of you've noticed that. So, we have a bunch of if-else statements. If you like some quote, follow through fast. The key thing is, we want to detect some small parts of the kind of images. Right? For example, we want to detect the color, we want to detect the shape, we want to detect the length. And we have so many different if-else statements, so finally to decide whether that is correct. But 1 question is, is that enough? How many those standard if-else statements do you needed to make? How many? You don't know right. Even than better, right? Because there are so many parts of the image, right? And this is the big. So that's very about some traditional programming, because traditional programming is, we're not a care about the images, the computers classify itself, right? Only care about some small staff, how to make them together. But in this case, the Machine Learning is more likely think of a whole image, not just for example, or small parts.",
		"week": 1,
		"page": 39
	},
	{
		"transcript": "For example, can we manually write an algorithm or hard code that caters all the variations? Like traditional methods, can they do that? Probably very challenging. Right? Because different possess of cats. Right?",
		"week": 1,
		"page": 40
	},
	{
		"transcript": "Very challenging. I mean, different shape, strange shape. Somehow has the cat more like water. It can be in any shape. Right? So that means that you need to test your very challenge with that environment.",
		"week": 1,
		"page": 41
	},
	{
		"transcript": "So then, for the Machine Learning algorithms, an algorithm that will then follow from data or experience, and then there's no need to formulate explicit rules. In the algorithm, performance gets better with experience or data. If you have more and more data, definitely, your Machine Learning algorithm can output a good accuracy but we're heading about that. Even sometimes, you know you cannot get more data, but you can do some data augmentation. Try to generate some big data. You have also tried to get some better results. Then models will be allowed to get an application plan.",
		"week": 1,
		"page": 42
	},
	{
		"transcript": "So now, we talk about the question. So, what is the Machine Learning? And this is very about it. The early stages of computers in 1959, so the field of study that gives computers the ability to learn without being explicitly programmed.",
		"week": 1,
		"page": 43
	},
	{
		"transcript": "And this guy says that Machine Learning is the study of computer algorithms that allow computer programs to automatically improve through experience. And then in this case, we'll care more about automatically. Right? That is important because we want save our time because we are human beings. Right? And well lazy, we cannot do everything, we want some computer to do that.",
		"week": 1,
		"page": 44
	},
	{
		"transcript": "And the Machine Learning algorithms ingest data and model, like, more like the hypothesis. So, the learned model can be used to detect the patterns, detect the trends, detect the structures, etc. from data set. And then we make predictions on unseen or the new data set. And the letter when we will talk more about those unknown applications.",
		"week": 1,
		"page": 45
	},
	{
		"transcript": "The next question is, 'Why study Machine Learning?' Why is it important? Here's the key thing that we want to develop a better computer system to help us. So, to develop computer systems that are to, that is difficult and inexpensive to construct them manually because they require a specific detailed skills or knowledge tuned to a specific task. As usually the knowledge engineering bottleneck, if you have your own data set, data set especially in industry, every company even has their own data set. Right? In that case, you need to build your own model again, create some labels, retrain this situation that you cannot prevent. So, this is called the bottleneck. Why do we have that? To develop systems that can automatically adapt and customize themselves to individual users. For example, personalized news, the mail filtering. For example, sometimes, you have some app, and give you some exceeding news that you care about that. Right? But keep give your some coming out in this file. And also in your mail, so every day you get lots of spam email. Right? So, you can also detect that. Nothing is about personalized tutoring. We also can discover new knowledge from large database. This is really about the data mining part. For example, about the market basket analysis. And about the medical text mining to get some most common past experience, or whether you have some diseases, some kind of information, some drugs you take. Right? So, that can give you personal recommendation for that.",
		"week": 1,
		"page": 46
	},
	{
		"transcript": "So, why Machine Learning is necessary? So, learning is the hallmark of intelligence. So, many would argue that a system that cannot learn is not intelligent. So, actually, without learning, everything is new, right? A system that cannot learn is not efficient because it reloads each solution and repeatedly makes the same mistakes. So, why is the learning possible? Because there are regularities in the world. You can find some common patterns between a lot of them. We have a data problem. Right?",
		"week": 1,
		"page": 47
	},
	{
		"transcript": "You will try to use Machine Learning. For example, in the speech recognition. For example, you'll use Siri and ask her to do something for you. Right? To customize, to find something for you like in the speech recognition.",
		"week": 1,
		"page": 48
	},
	{
		"transcript": "And also, in the visual recognition with Machine Learning to try to recognize whether this is a dog or whether it's a cat. Okay.",
		"week": 1,
		"page": 49
	},
	{
		"transcript": "And also, about the face detection to recognize where in the face, again, those kinds of bounding box. Right? And also, about the expression recognition of whether you are happy or whether you are sad, right? We have some different things.",
		"week": 1,
		"page": 50
	},
	{
		"transcript": "And also, we can recognize the handwritten digits. For example, this kind of very weird. Right? It is weird. Right?",
		"week": 1,
		"page": 51
	},
	{
		"transcript": "And also, some models are customized, as I've said before. For example, the personalized medicine, the personalized recommendation, like a home assistant, they are uniquely, and we are uniquely. Everybody is a unique person. Right? We have lots of human races. Right? So, that in the future, the personalized stuff will be very important for all of us. Models are based on a huge amount of data sets that you guys already mentioned that. For example, like genomics. Like a stock price prediction.",
		"week": 1,
		"page": 52
	},
	{
		"transcript": "And also, like, self-driving cars care about autonomous driving. Right? So, this is all about some bigger data stuff.",
		"week": 1,
		"page": 53
	},
	{
		"transcript": "And even we can use Machine Learning to help us explore something that not exist. For example, the navigation task because our human has never been there before, right? Who can use some robot machine help us be explored first. Right?",
		"week": 1,
		"page": 54
	},
	{
		"transcript": "You care about the human capacity to be augmented. Maybe use Machine Learning tools. This is whether, oh, this part is really the corpus callosum. Remember my previous slide, this part is really about the corpus callosum. This part, this part. So, this part is actually related to memory. Now, that you will have some kind of increase so in parts of it. Let's say in the future, we will find those parts if they shrink, you might have some problem with that. Okay.",
		"week": 1,
		"page": 55
	},
	{
		"transcript": "Another trend is about why Machine Learning models are so popular. Right? Because there are so many places that we needed to use Machine Learning. For example, in the medical images that I mentioned before, in the speech recognition, and even in the robotics. And that, especially in that computer vision field, like the personal identification, the activity recognitions, some object detection, and the autonomous driving. And then also there's another key reason that we wanna use Machine Learning as we want to include some existing Machine Learning models. Right? And also, there are availability of large volumes of datasets, and we want to make a sense of data. So, nowadays, it's very easy to get a lot of data sets. Right? There's a lot of images from your phone. But do you want to segment your images on your phone? Do you want to do that? Then you find an object in your phone image. No one wants to do that. It's not necessary. Right? So, in your data set, if you can't do label it, there is no sense in this data. And also, that CRM custom customized software. For example, the speech recognition, software or spam filter, color detector, and all of those part that so we want to use Machine Learning, and this is the key reason why Machine Learning nowadays is so popular and important.",
		"week": 1,
		"page": 56
	},
	{
		"transcript": "And also, there are lots of related disciplines. For example, it is directly related to the AI. I think, you guys, we have another class for the Artificial Intelligence, probably, I will teach it next semester. It depends on whether it is we have a lot, we have more professors become my colleagues, but anyway, you guys need to take this class. And probably in the future, you need to take another class, called Reinforcement Learning. That is close to this Artificial Intelligence. Another discipline called Data Mining, and all of you guys are familiar with that, and about the probability and the statistics, information theory, and numerical optimization, the computational complexity theory, and control theory, psychology, neurobiology, linguistics, and the philosophy. All those different disciplines that, remember, need the Machine Learning algorithm to have us.",
		"week": 1,
		"page": 57
	},
	{
		"transcript": "So, now I want to talk about the taxonomy of Machine Learning. We typically have around this kind of 4 different aspects. Like the Supervised Learning, the Unsupervised Learning, the Semi-supervised Learning, and the Reinforcement Learning, another category of Machine Learning. For the Supervised Learning, we will use labeled data set. Right? And Unsupervised Learning, just discover patterns in unlabeled data set. There are no labels for data set. And for the Semi-supervised Learning, you will get trend of a small portion of your labels for data set. But for the Reinforcement Learning, we will not talk about labels or no labels, but we will care about how did you act based on the feedback or co-called reward from your environment. Like, for example, the play GO, here.",
		"week": 1,
		"page": 58
	},
	{
		"transcript": "Is this slide is more about, it's just classified into the three. And in the previous slide, we had Semi-supervised, another kind between these ones. Other majority, we have these 3 different categories. For example, in the Supervised Learning, we will have our typically known tasks like Classification, Regression, and a Ranking problem. For the Unsupervised Learning, we will have, like, the Clustering, the Dimensionality Reduction, the Unsupervised Density Estimation. And the lastly about the Reinforcement Learning. So, after Reinforcement Learning, firstly, doesn't use labeled unlabeled data in the traditional sense. In the Reinforcement Learning, an agent learns via its interactions with an environment, so-called feedback-driven policy learning. So, we will care about its feedback from the interactions between your environment. And there are also, let me come out with any message that I need to mention, let's say, the Semi-supervised learning that we've already mentioned. And we have Active Learning. We have Transfer Learning. We have the Multitask Learning. We have, like, Imitation Learning. So here, a kind of related to the Reinforcement Learning. And we have Few-Shot Learning in nowadays, and I don't know whether some of you have heard about that or not. And even we have the Zero-Shot Learning in Machine Learning. We probably have no labels or data set but we still can predict them, so-called the Zero-Shot Learning. This is a kind of fancying learning algorithms, but we'll not cover in this class. You probably will see it in your future classes.",
		"week": 1,
		"page": 59
	},
	{
		"transcript": "So, next thing is about the details of Supervised Learning. For example, we have numerical classifier functions, like linear classifier, the perceptron, especially about logistic regression. I think, all you should know about from your first semester, right, about this logistic regression, about the support vector machines (SVM), and also about the neural networks. So, all of those are numerical classifier functions, and you will have some parametric functions, parametric or probabilistic. Specifically, they are related to some probability kink of function, like Naive Bayes discriminator, Gaussian discriminant analysis the GDA, the HMN, the hidden Markov models, the probabilistic graphical models. And we have non-parametric models. So, this is related to some instance-based functions. For the non-parametric models, they need just that those models very depend on data set. They don't care about some kind of parameters. For example, the knn the k-nearest neighbors. It cares about its nearby instance, right? The kernel regression, and also similar to kernel density estimation, some local regression. And we have, we have the symbolic functions, like the decision trees, the classification, the regression trees. Right? Decision trees for as I can apply for this period to the task list. And we have the ensemble learning, like the bagging, the boosting as the Adaboost and random forest. For majority of those kinds of methods, we will cover the rest of the semester, you'll see about that.",
		"week": 1,
		"page": 60
	},
	{
		"transcript": "What about another category is Unsupervised Learning, like categories and techniques. For example, Clustering, the k-means clustering, the Mean-shift clustering, the Spectral clustering. The Density estimation function, like GMM the Gaussian mixture model, the Graphical model. And the data dimensionality, the Dimensionality reduction method, like the PCA, like Factor analysis. So, all of those are some Unsupervised Learning.",
		"week": 1,
		"page": 61
	},
	{
		"transcript": "Professor: Now, let's try to talk some framework of existing Supervised Learning and Unsupervised Learning. You know about Reinforcement Learning. Right? So, given a bunch of images of training data set, labeled data set, let's find these images like in the box, the cat. After we used some kind of feature extraction, and with its labels, we will try to train some Machine Learning algorithms. And after that, we can try a good predictor to detect whether it's cat or whether it's dog. Right? So then, keeping any test image. Also, after feature extraction, we can get some predicted labels. Right? However, why important the process is called feature extraction. What does that mean? And here, what is feature extraction? Student 1: Taking the specific qualities of that content, but we are giving those labels. Professor: Yes. Any other thoughts about feature extraction? Student 2: We have some data, we have some information, but we're using some methods. We can get some more information from the data, some additional information. Professor: Yes. It can be yes, exact! Specifically, more about the feature extraction is like some kind of method to extract some key features from data. So, here, it is images. The key features from images. For example, those cats, if you want to extract some corners, some conture information, some boundary information. So, those are so-called the super power features that we can extract. But, in our machine learning class, we will not talk about how to extract features. But in our another different learning class, we will talk about how to extract those deep features or in the traditional features. I think, maybe all you, guys, should take that Deep Learning class. Right? I guess, you've already registered. Right? You already did that. Yeah? Last semester. Yeah. But again, for the feature extraction, you guys will learn in tomorrow's class. Okay. So, later you will get more sensitive of it.",
		"week": 1,
		"page": 62
	},
	{
		"transcript": "Another task about the Unsupervised Learning. What role for clustering? For example, in this case, you have no labels about the images.  But here, previously, you had also those cats, so this is called labels. Right? In this case, you have no labels. You just have images. Right? After you do feature extraction, and can get this kind of one-dimensional vector for each image, and also you try to train some machine learning algorithm. You have, like, these different clusters. So, this is really based on some clustery method. Right? For example, that like some knn, k-nearest neighbor. You will probably choose, the given the K is equal to 1, you will definitely choose the most of closed samples as in the same category. So, for example, if these two cats are very close to each other, you say, you're thinking they should be in 1 category right, and you can put it here. But if you find this cat and this dog, they are different from each other, like distance between these 3 images is big. So definitely, you need a support into 2 different classes. Right? So eventually, you can't get those two different classes. That is co-called Unsupervised Learning.",
		"week": 1,
		"page": 63
	},
	{
		"transcript": "Lastly, I want to briefly mention about Reinforcement Learning. So actually, this is about overall framework of Reinforcement Learning. As you can see here, you'll probably have your environment state. You have several states for that state S. If you try move, so which state. And for example, you will play this little game, you have some kind of observations whether you can jump, whether you can feed something, whether you can eat something. Right? And you will get some reward, so rewarding typically about your score. So, if you can jump some obstacles, you will get a high score, right? So that is so-called reward in this case. The action, action doing, in this case, is maybe just about to move forward, move back, or jump, right? Those support that 4 different actions that you can perform in this task. And out there, you have some agent state. So, after you perform a kind of action, you will jump to another state. For example, if you take action for walk, you can move to that forward step, right? You make some reward; you'll get some score or you'll lose score. So that's probably reward here. But anyway, you guys will learn more details about that in the AI, Artificial Intelligence course or Reinforcement Learning class. And specifically, for one student who says that he's interested in gaming, this is very important for you, these two classes. The AI and this Reinforcement Learning. But sometimes, usually for this AI or Reinforcement Learning, for the homework, you guys have some kind of gap, to place some gap, whether you can win or not.",
		"week": 1,
		"page": 64
	},
	{
		"transcript": "And also, this is about the traditional flow work for the classification. And this is exactly we need to learn in this class. So, in this class, we'll only utilize a kind of traditional machine learning models. For example, I like the Decision Tree, the SVM, the KNN, the MLPs, the Naive Bayes. So, those all of those traditional methods. And that this is the problem, something called the feature extraction that I've mentioned before. For example, you will try to extract the SIFT feature, the HoG feature, SURF features. So, in the tomorrow's class, we will more care about the feature extraction model.",
		"week": 1,
		"page": 65
	},
	{
		"transcript": "And here's about lots of machine learning algorithms. For example, by the Numerical functions, we have linear regression, numerical neural networks, support vector machine. And the Symbolic functions, we have Decision trees, propositional logic, rules for first-order predicate logic. And we have for the Instance-based functions, we have k-nearest-neighbor and the case-based reasoning. And we have for the Probabilistic Graphical Models, we will talk about Naive Bayes, and Bayesian networks, the HMM, and even about Markov network. So, this is about awesome kind of machine learning algorithms, but again due to the limit time in the fall semester, we won't able to cover all the algorithms. But I will emphasize some very significant one, and some, someone I will ask you to do for your future interview, those algorithms.",
		"week": 1,
		"page": 66
	},
	{
		"transcript": "So then, we will move to the application field.",
		"week": 1,
		"page": 67
	},
	{
		"transcript": "For example, this is very about the medical disease diagnosis.  In the left, for about this image.  Any idea about this image?  Yes.  Actually, an x-ray of your lung units. Right? So, in this case, you can diagnose whether you have Covid or not. So, there are already a lot of publications that talk about whether diagnose whether you have or not from x-ray images.",
		"week": 1,
		"page": 68
	},
	{
		"transcript": "And we also have this person identification. So, give your face images while I try to detect where are your eyes, where is your nose, where is your mouth. Right? It's funny areas. Right? It's really about person identification. So, for your iPhone, then very we'll use those kinds of important points to unlock it.",
		"week": 1,
		"page": 69
	},
	{
		"transcript": "And lastly, it's about the biometric, about using the fingerprint. So, actually before we use that kind of squeeze to unlock home using your finger. Right? In the early world, at Windows like 2006 or something like that. And then a world, you will use your finger print to unlock your home. Right? Is that true?",
		"week": 1,
		"page": 70
	},
	{
		"transcript": "And even we can try to analyze your posture, I mean this kind. In the left hand is about a human and the right hand just tries to show a posture.",
		"week": 1,
		"page": 71
	},
	{
		"transcript": "And we also have another semantic recommendation about lab aids, just images. On the right hand, just a couple of some labels of these different objects like the cars, the roads, the trees. Right? All different kinds of labels, yes?",
		"week": 1,
		"page": 72
	},
	{
		"transcript": "Also, there is another one about the medical image segmentation. So, the left one just is one MRI image. So, left hand, we try to segment this white matter of perimeter of it. That is the blue color. The center has the yellow color and the red color here.",
		"week": 1,
		"page": 73
	},
	{
		"transcript": "Now, also, for those care about the stock. So, this is about the stock price prediction. Albeit. Right? I can also use Machine Learning to do that.",
		"week": 1,
		"page": 74
	},
	{
		"transcript": "And also, there are lots of revolution in the robotics nowadays. Right?",
		"week": 1,
		"page": 75
	},
	{
		"transcript": "So, there are lots of different robotics use the machine learning. To for example, there are several areas that we can use the robots. Like industry robots, the military, the governments, and the space exploration robots, service robot from home, the healthcare, electric. So, why are the robots used? Because some tasks, it is very dangerous for human beings. Right? And there's some repetitive tasks for some tasking you will repeatedly do it. So, human is not able to do it. And another way is by high precision tasks. All those who require high quality, like to make some GPUs. Definitely, you need a machine instead of human. Is that right? And even to save your time. And the control technologies. For example, autonomous driving, also like another thing about remote control. Right?",
		"week": 1,
		"page": 76
	},
	{
		"transcript": "So, here's about several frequently used the robotics in the industry like the painting, the cutting, some assembling, some material handling, some like packaging, machine loading, all kinds of different robotics. They have some machine learning algorithm inside. For example, like packaging, the first need is to recognize objects. So, then you can package those objects.",
		"week": 1,
		"page": 77
	},
	{
		"transcript": "And this is the users for robotics in an inventory of manufacturing like the automotive and also about is this packaging staff.",
		"week": 1,
		"page": 78
	},
	{
		"transcript": "So, here's just about some automotive kind of machine here.",
		"week": 1,
		"page": 79
	},
	{
		"transcript": "And also, there are other kinds of military or government robots as 'iRobot PackBot' a kind of bot, and the 'Remotec Andros' about those different machines.",
		"week": 1,
		"page": 80
	},
	{
		"transcript": "And even about soldiers in a kind of world, they try to use some robot, do that. And even now this, they are famous. So, a kind of them that are still that are working in Ukraine. Probably you heard about that, they use drones, like, trained to do that. Right?",
		"week": 1,
		"page": 81
	},
	{
		"transcript": "A soldier robot throws some bombs to human being. So, this is like, I'd say like the UAV, the aerial drones. And also, about the military suit. All about this case try to do some detection some problem of it.",
		"week": 1,
		"page": 82
	},
	{
		"transcript": "And also, you know about the space robot that I mentioned before to try to explore the Mars. Right?",
		"week": 1,
		"page": 83
	},
	{
		"transcript": "And also, there are lots of other service robots. You maybe have it at home or in the office, by cleaning and housekeeping. By others, so, many inspections, blah blah, there are some kind of robots you might have already seen that. However, for each robot, there are lots of machine learning algorithms, a kind of. Like for this kind of cleaning robot, that has a sensor to present how to decide what is distance between different objects. Right? Definitely, you want to avoid some objects at home, you do not want to break it. Either to break the object or to break the robot itself. What about this cleaner? The algorithm in this area is just very similar. It just only needed to get a sense of distance. Right?",
		"week": 1,
		"page": 84
	},
	{
		"transcript": "And then, many medical and healthcare applications. So, nowadays, there is more and more applications that have been heavily applied in the medical field. Like you see. Once you tried with a hospital, it was a lot of machines, which catch it with you. Right? Here is about your heart rate or other kind of measurement. All of them are machines, right?",
		"week": 1,
		"page": 85
	},
	{
		"transcript": "And also, we have some laboratory applications like the drug discovery, a test tube sorting. So, all, about can be some useful to us in a laboratory.",
		"week": 1,
		"page": 86
	},
	{
		"transcript": "And this is the famous sentence from Bill Gates that 'A Breakthrough in Machine Learning will be worth ten Microsofts'. Is that true? I think, right?",
		"week": 1,
		"page": 87
	},
	{
		"transcript": "I guess, this many the most content that we have today. So next, probably I will talk some kind of potential work you guys can do when you try to finish this course, a typical street kind of open job.",
		"week": 1,
		"page": 88
	},
	{
		"transcript": "And you, you definitely have more chance to get these three that I list. For example, the Artificial Intelligence engineering, some Machine Learning engineering, some Data analyst, all those kinds of job that you guys can try to apply. But again, you know, first of all, in the view, you guys definitely need to learn about the coding stuff. That's all about basically about Machine Learning. Questions? Right? What is Supervised Learning? What is Unsupervised Learning? That's a very easy question. They'll ask you; how do you prevent the overfitting? How do you implement the KNN stuff like that? For example, you guys have such a lot of those questions you'd like Machine Learning. And then that's the key reason, you guys need to study this course carefully, because it'll directly affect your future. Hopefully, guys you can learn a lot from this class. What is next?",
		"week": 1,
		"page": 89
	},
	{
		"transcript": "Next, actually, I have the first homework for you guys. The first homework is very easy. Right? It's just about, asking about thinking about; What is machine learning and what is the future of machine learning? Easy, right? Remember, use your own sentence. Okay? Because I have another task for you guys. That you guys probably heard about ChatGPT. Right? That's perfect. Right? All you guys heard about that. You guys need to try to use ChatGPT. Any of you have already tried that? You tried that? What about you guys? Don't? So, now you guys have chance, officially, revenue in this class. I will require you guys also to provide some answers from ChatGPT. Answers. Okay? So basically, which means that you will upload two files, two text files. Include a question and your answer. For example, your question just; What is Machine Learning and what is the future of Machine Learning? But for that ChatGPT you must modify these questions a little bit. Because it can give you some different answers. You just, you definitely want to get different answers from your classmates. Right? But the case about your own answer, always remember to write your own sentence, because in this time, I will check it, use online, a kind of detection tool, if you have come some sentence a copy from online. You probably have some trouble with that. I think you guys have already covered about academic integrity. During the orientation. Right? So, that is very important. I do not want to say that details of it, but again, try to remember it. Do not copy it from online, do not copy it from your classmates. Okay? It's okay, it's easy. Yes. Yes. Exactly. You needed to upload two text files. Right?",
		"week": 1,
		"page": 90
	},
	{
		"transcript": "So, for example, here is the example from me, from me. And since this is for the example file from ChatGPT. But question; What is machine learning and how machine learning can change the future? And this is the solution that it gives to me. So, it says, Machine Learning is blah, blah, blah, and the Machine learning has the potential to change the future in a number of ways. Some of you might get exactly the same answer. But that's fine. I do not want you guys to modify the answer from the ChatGPT. The case, I want to do that. The one thing that I want to ensure you guys not to copy from online. Especially, now if ChatGPT is very famous, very popular. That already some professor noted that some students use this right at their homework then submit it. So, now I directly want you guys to do that. So, that I can very verify whether it is you or you got a machine. You definitely finally, you after you finish this class, you will not say, well, you cannot beat a machine. Right? You cannot beat the ChatGPT with a good answer. Right? Try to avoid that. Okay? Others. Probably that's about my last slide. I hope you guys have any questions regarded to anything. Okay? Okay?",
		"week": 1,
		"page": 91
	},
	{
		"transcript": "Any more questions? What about our online students? Any questions about you guys? Any question? No? Okay. What about the homework? Otherwise, clear about? Okay. That's good. If not, so then, our class is finished today, and you guys have a good night, and now I hope I'll see more of you next week. Okay? Thank you. Thank you, sir.",
		"week": 1,
		"page": 92
	},
	{
		"transcript": "Let's get started for our today's class. Again, for that 5-piece interview, it doesn't matter if you didn't finish all of it. It's fine. But once you finish the machine learning class, make sure you are able to do it within, like, the 30 or 20 minutes. It is like a real kind of practice of the coding problem from the interview.",
		"week": 2,
		"page": 1
	},
	{
		"transcript": "Professor: So, next, let's go first slide that I bought it. It's about how to find a job in the Machine Learning. I guess, that should be most important part for you guys, right? Of all you want to find a job, right? And that case is about practice and practice. As you guys already did, like, a small interview practice, like, some similar concept. And I don't know whether you guys know about Leetcode. Anyone knows about it? No? For those who didn’t know about this website, you can try to and we can open with advice. This is the real about website that has lots of questions, especially, it contains some interview questions for your guys. Can you see now? So, this is the website of Leetcode. So, after there are lots of, like, questions, like, especially if you are interested in the Python, you guys just look at Python, you can you are interested in other languages, like C, C++ or even Java, there is lots of resources. You can do that. And I think, it should be a good time for you guys to start a bit with it. Especially, and it has lots of interview questions for those top companies, like from Google, from Meta, from Amazon, there are lots of questions, interview questions, to make sure you have time to practice with it, to challenge yourself. I think, you guys might get a similar interview question during your future interview. I guess, here, I like well, lots of questions. You needed to take a run like one year or one year half to fish all of them, but please make sure that you select the most useful questions for you guys. If you target in the future, you're really about to find a job immediately. I highly recommend you guys to start to play with it. Okay. Yes. Oh, for those online students, can you see the screen? Student 1: Yes. Student 2: Yes, professor. Professor: Can you see the screen about the Leetcode? Student 3: No, we only see the slide. Professor: But I think there should be a link in the slide. Right? So, later, to print and make sure you look at it and really start to work on some questions. Okay? So, this is about, I think it should be one major task for you guys in the future, right?",
		"week": 2,
		"page": 2
	},
	{
		"transcript": "Another kind of direction is ready. Probably, one wants to pursue, like a PhD degree, right? And there are definitely several reasons. The first reason like typical you will focus on doing some professional Research, especially you will become a professional researcher here, and then you definitely will practice with some cutting-edge technologies. Some state of art methods. It is not like the main country. It is some of the industry companies they are old. Still are using some techniques around, like, 10 years ago or even 30 years ago. And now, if you really want to pursue the PhD degree, you definitely need to do some state of art methods. And secondly, definitely, if you publish some papers, the publications will be kept forever. Right? So, eventually 180 years later, you can show something to the world. Right? The second reason, definitely, once you get a PhD degree, you can get a much higher salary, this should be true. However, saying that if bachelor students or master students use another 5 years, you'll probably have a similar salary which means that typically for a PhD degree, you need, like, 5 years to get it. But at the beginning salary should be similar to another, like, bachelor students with 5 years of experience. Right? However, for a PhD degree, you definitely have better career prospects. For example, in many companies for master’s students or bachelor’s students, you probably cannot be a manager. But if you hold a PhD degree, you'll probably become a manager. So, that's really about talk about your future career. Like for the final goal, right? And the final destination, right? But to get a PhD degree, you will apply for a PhD degree, you definitely needed to focus on the research. Right? So, this is very important for you guys for those who are interested doing research. And then you want probably, if you still want to apply for a PhD degree. So, if you want to do some research. It generates the funding, and we try to give you some kind of projects for your better experience if you apply for a PhD degree. Okey? So, this is about the best security directions in the future. After this machine, machine learning class, you can do it here.",
		"week": 2,
		"page": 3
	},
	{
		"transcript": "And today, we really want to review some basic math knowledge and about the data the data preprocessing. In the first part, we will really talk about this kind of math knowledge. For example, we will talk about the Linear Algebra, we'll talk about Vectors, Matrices, the Eigen decomposition, and we will talk about the Differential calculus, the Optimization Algorithms, Probability, Random variables, Probability distributions and the Information theory. And those are some basic maths. I believe most of you should remember or notice about that. So, if you join the class, you find some concepts you are ready forgot. At least refreshing your memory, right? You can go to, go to online and good buy it. What are they right now?",
		"week": 2,
		"page": 4
	},
	{
		"transcript": "And the second part is really about the Data Preprocessing, and we will talk about some Data Cleaning, Data Integration, Data Reduction, and Data Transformation and Data Discretization. And something you guys already did in the today's practice question. For example, Data Cleaning like I requested you guys to replace the question marker with a number about so this is really about Data Preprocessing. But unfortunately, we may not able to cover all the topics of the lesson. Let's see which we can get there.",
		"week": 2,
		"page": 5
	},
	{
		"transcript": "So, first, let's do the notation part. And so, for the first several like 6, these should be very easy to remember, and actually you guys should already notice about that. Right? Some integer like a scalar, xyz with the bold-front, lowercase, it is a vector. And ABC, it should be like matrix, and this kind of ABC should be a tensor. So, that we will be also covered. Then, we have random variable XYZ, and we have one element in a set. Right? So, this is about the number of items in set A. And this is the norm of the vector variable. This is the product of u and v. And this is very missed set of real numbers. And this is the real numbers in the dimension of n. And also, this is about a function map from x to the f(X). And here is the function that maps the n dimensional vector to a single scale. Any question about those kinds of notation? Okay? Okay. Great.",
		"week": 2,
		"page": 6
	},
	{
		"transcript": "And we have this one like the circle dot with B is the element wise product of the matrices A and B. And also, we have this A+ here, it has this pseudo-inverse of matrix A. With here, it's about the n-th derivative of function f with respect to x. And here's about this gradient. This is about the Hessian matrix of the function f. X~P is a random variable X has distribution P here. P(X|Y) is already about a probability of X given Y. This N is the normal distribution. And we have this expectation of f(x) with respect to that P(x) here. And then we have the variance of f(X). And here's about covariance. And here's about correlation. And here's about the KL divergence as in this you probably haven't heard not heard about it before. Right? Did anyone hear about this KL divergence before? Not here? That's fine. So, we'll like cover it about what is KL divergence, but it's fine. It's a kind of divergence can minimize the distribution about P and Q. And here is about cross-entropy for distributions P and Q. And this should be an exact formula. Right? There are also about other important notations.Any questions about those notations? No? Okay. That's good. So, basically, about these two slides, we will cover around the majority of notations that we will cover in the pool of machine learning class. If you're thinking about already, you know about these notations, you will be fine in this class.",
		"week": 2,
		"page": 7
	},
	{
		"transcript": "So, let's review about the first part math about vector. So, what is the definition of vector? So, that is, you know, in computer science, vector is a one-dimensional array of ordered real-valued scalars. In the mathematics, the vector is a quantity possessing both magnitude and direction, represented by an arrow indicating the direction, and the length of which is proportional to the magnitude. So, vectors are written in column form or in the row form denoted by this x in this case. So, this is the x, it's equal to 1, 7, 0, 1, and also, we can denote this x using a transpose. Right? Okay. So, for a general form vector with n elements, the vector lies in the n-dimensional space that is x should be what elements in the R, with n dimensions. Alright? As you can see here, so that has n elements here.",
		"week": 2,
		"page": 8
	},
	{
		"transcript": "So, let's see, you see some details of the geometry of vectors. So, for example, the first interpretation of a vector that is a point in 2D space in this case. Right? For example, in 2D space, we can visualize the data points with respect to a coordinate origin. For example, we have like these three kinds of points here. Right? And in the second interpretation of a vector, the direction in space. For example, the vector v is equal to 3 and 2, it's okay. Often, we are writing its transpose at the direction of 3 steps in the right and the 2 steps up as you can see probably here. And there's the notation v, like with the head, is sometimes used to indicate that the vector has a direction. Right? So, all the vectors in the figure, in this case, they have the same direction. What about the vector addition? So, we add the coordinates, and follow the directions given by the two vectors that are added. For example, we have this, this is the sum of these 2 vectors. Right? Any question about it? Good.",
		"week": 2,
		"page": 9
	},
	{
		"transcript": "So, the geometric interpretation of vectors as points in space allow us to consider a training set of input examples in Machine Learning as a collection of points in space. For example, in this particular bunch of points like these points. For example, let's say if you need a classification problem, we really, we want to discover how to separate two clusters of points belonging to different classes. In this left. For example, we end up finding this kind of line. And this line is called what? Student 1: this line is called what? Professor: Any guess about this line? Student 1: Of what? Professor: What? Student 2: The model? Professor: The model? Kind of like a model. So, what about this line? Any other guess? Student 3: Sorry, professor. Could you please, could you, could you please repeat question? Professor: Oh, the question is, so, what do you think about this line? In the Machine Learning, it's another term about this line. Say it again. Student 4: Regression? Professor: Regression? A kind of, but not exactly. Student 5: Classification line? Student 2: Task classification line. Professor: It's a kind of close. So, actually, this line has, I think about, like a hyperplane here, right? So, later we will cover about the hyperplane. There is a plane to separate these two classes. But here, it's not about lines. It's just a kind of a plane. But later, we will cover more details about it, what is a hyperplane, how to separate these two different classes, right? Or, it can improve, it can help to visualize zero-centering and normalization of training data. So, in this case. So, for example, this red is the original data. So, the middle one is the zero-centered data, and the right one is normalized data. What does this zero-centered data mean? Anyone about it what is the zero-centered data? Anyone knows about this meaning of the zero-centered data? What does this mean? You can try to compare about the left, this one and this one. You see any changes between this blue one and the green one? Student 6: In the center, the data is close to the zero. Professor: Yes, you have a good catch about that. So, basically, the zero-centered data, the mean of the data is equal to 0. So, that you can see that. This data is shaped to the center that is so called zero-center. What about normalize for that listening? Student 3: And I think. Professor: Kind of like that. Sure. You want to, like, convert your data to some kind of skill. Right? Why do you want to do normalization? Why do you want to do that? Student 2: Because we have outliers. Professor: Outliers? Student 2: Outliers, prevent from our data. Professor: A kind of. What about another guess? Why we want to do that? Yes, something like that. It's better. Okay. So, the reason we want to normalization is that we want to unify our data in some scale. For example, we will typically, we try to normalize our data between minus 1 and 1, right? So, that you can see the majority of data set. And another case, it is useful for your Machine Learning, particularly in that. The normalized data you probably use less time to change, right? Because, think about it, if you input the original data, the scale of your data set can be wide, right? It can be very big. However, you can use, you use the normalized data. So, in that case, you will adjust the data set, concentrate with each other, rights? So, that you will have less time to train your model. That is another advantage to this normalized data. Okay.",
		"week": 2,
		"page": 10
	},
	{
		"transcript": "Let's continue, the dot product and the angles. So, the dot product of vectors that is donated in this equation like u times with v and that is u transpose with v. And basically, this sum of all the elements. Right? It is also referred to as inner product, or scalar product of vectors. The dot product u times v is often denoted by u and v using this kind of brackets here. The dot, dot is a symmetric operation. This means that u times which v is equal to u transpose with v is that it's equal to v transpose with u, and also is equal to v times with you. Right? So, this already means about the symmetric operation. The geometric interpretation of the dot product; angle between 2 vectors. So, you're investing, this is about 2 different points, and here, a theta is just, just the angle between these 2 vectors. Right? And also, we can calculate by the dot product of v, the dot thought of this value that is using this equation based about what, what is this? The norm. Right? This is the norm of u and v times to the cosine theta. And also, the cosine theta is calculated by this equation, right, u dot with v over these two norms, the production of this. So, if 2 vectors orthogonal, which means the theta is just equal to 90. That cosine theta is equal to 0, then we can u times with b, that is equal to 0. Also, in the Machine Learning that cosine theta is very important. Because, sometimes it just means the cosine similarity so that we can measure the similarity between two vectors. For example, in the NLP, giving you 2 different entities, like 2 different texts. You want to calculate how similar of these two taxes where you want to calculate the cosine. So, now, another case of it, computer vision of artificial learning like give you three images, you want to measure how similar are they. So, also, you can calculate the cosine theta of these three images. So, that you have an idea of how similar of these three images. Right?",
		"week": 2,
		"page": 11
	},
	{
		"transcript": "Our vector norm is a function that maps a vector to a scalar value. So, the norm is the measure of the size of the vector. The norm of f should satisfy the following properties. For example, about the scaling, about the triangle inequality, and it must be non-negative. So, which means the fx should be bigger or equal to 0. So, generally, we have this l_p norm of a vector x is obtained as. So, in the next page we will really talk about the p value from one to two through to infinity.",
		"week": 2,
		"page": 12
	},
	{
		"transcript": "And then if the p is equal to 2, which means that we have the l2 here, also called the Euclidean norm. As you can see in this equation. So, it is the most frequently used norm. So, l2 norm is often denoted just as the norm of x with the subscript 2 omitted there. So, typically, we will try to omit this too. Just to use this, denote this as a norm. If p is equal to 1, so we will call it as l1 norm. And they use the absolute value of the elements, and we can discriminate between zero and non-zero elements. So, actually, in the future, especially about some regression problem, you probably will use l2 norm or l1 norm, to try optimize them or your models. Let's say p is equal to the infinity. So, we have l∞ norm. So, it's known as infinity norm here or maximum norm, as you can see in this screen. And we will output the absolute value of the largest element. However, if p is equal to 0, some of you might have this kind of question. So, really, we cannot say it is like the zero norm. Is not a norm at all as you can notice. Right? Any questions? No questions? Good.",
		"week": 2,
		"page": 13
	},
	{
		"transcript": "Let's continue. What about the vector projection? So, the orthogonal projection of a vector y onto vector x. So, the projection can take place in any space of dimensions that is bigger or equal to 2. So, the unit vector, which means that in the direction of x that is denoted in this way. So, typically the length of the unit vector is equal to 1. Right? And the length of the project of y onto the x is denoted in this screen that is the y norm, the norm of y times cosine theta. And the orthogonal projection is the vector that we can denote in the different and specific formula right here.",
		"week": 2,
		"page": 14
	},
	{
		"transcript": "So, now, really, let's go back to what is the hyperplane that I previously just mentioned. So, a hyperplane is a subspace whose dimension is one less than that of its ambient space. For example, we have 2D space. I have a hyperplane used as a straight line that is 1D. For example, in this case, because this is about a 2D space. So, a hyperplane is just a line. Right? In 3D, so, our hyperplane is a plan. So, typically, it's like a surface. As you can see in this example. So, this is 3D, and our hyperplane is really about a surface. Right? In a d-dimensional vector space, a hyperplane has 𝑑 − 1 dimensions, and divides the space into two halves of space. Now hyperplane is a generalization and a concept of training in high dimensional space. As you already have noticed that. In Machine Learning happens, you see boundaries. You use them for the linear classification as you can see in these examples. Data points falling in either side of the hyperplane are attributed to different classes. For example, if you are in the left, it should be one class. If you are in the right of the hyperplane, it should be another class. Right?",
		"week": 2,
		"page": 15
	},
	{
		"transcript": "So, now let's see the details about these examples in the hyperplanes. For example, for a given data point, that is w quals to 2, 1. So, there is 2 and 1 here. This is a vector here. We can use the dot product to find a hyperplane for which there is w dot v is equal to 1. So, it's very, in this case, it's about 1. Right? So, that if you say all vectors, with w times with v that is even bigger than 1 can be classified as one class. So, this part. And all vectors that is less than 1 can be classified as another class, and this is typical about our classification problem. If we want to solve in the w dot b, that is equal to 1, we will obtain this kind of equation, right, but eventually, we will get the norm of v, the cosine theta that is equal to 1 over square root of 5. That is, to say, the solution is the set of the points which w dot v is equal to 1 meaning the points lay on the line that is orthogonal to the vector w. So, it has a typical symmetry in this case. And that is the line 2 of x plus y, that is equal to 1. And the orthogonal projection of v onto w, that is a norm of v, the cosine theta that equals to this 1 over a square root of 5. And this is really about hyperplanes. Right?",
		"week": 2,
		"page": 16
	},
	{
		"transcript": "And the previous this example, just in the 2D space. So, what about in the 3D space, and it should be very similar as before. But in this case, definitely, we have, like, the 3 different directions, that is 1, 2, 3 in the 3D space, and try to find all points that satisfy this kind of w, w dot with v that is equal to 1. We can obtain a plane that is orthogonal to the vector w. And you should satisfy these two kinds of options, and also in that case of that hyperplane. This is about really about the hyperplane into that surface that can satisfy the v times with w equal to 1. And then here, it's bigger than 1, and here's less than 1, that means two different classes. Right? For higher dimensions, this should be very similar to these 2 cases. Right? 2D or 3D, I imagine that more dimensions will be very similar to these two cases.",
		"week": 2,
		"page": 17
	},
	{
		"transcript": "So, next what we want to reveal about the metrics. So, a matrix is a rectangular array of real-valued scalars arranged in m horizontal rows and n vertical columns So, each element aij belongs to i-th row and j-th column. And the elements are denoted aij or capital Aij or these two. So, basically, these are about all elements in the matrix A. On the matrix A, it has the size of m by n. Right? So, the size or dimensions is m by n or like maybe this description. So, matrices are denoted by bold-font upper-case letters.",
		"week": 2,
		"page": 18
	},
	{
		"transcript": "Professor: What about the operations in the matrix? For example, addition or also about the subtraction. So, what are the answers for these few operations, these operations? These two matrices? What are your answers for it? What is answer here? Student 1: For addition? Professor: That's correct. That's always correct. What about the scalar multiplication? It's just like we can pull this to stay out of all the metrics. What about this number? Student 2: You need to pass the value to the right. Professor: So, what about the matrix multiplication? It's A row elements try to multiply with column elements of B matrix. What about the answer for this one? Yes. Two by two matrix. What is the final answer for it? So, this is gonna be fine. I guess, you should have noticed about simple operations of matrix. It's good.",
		"week": 2,
		"page": 19
	},
	{
		"transcript": "What about transpose of the matrix? So, A with T, it's transpose, has the rows and columns exchanged. Right? So, this is the transpose of this matrix, should be equal to this one. Right? There are also some other properties such as A plus B is equal to B plus A. So, A + B with its transpose should be equal to A transpose plus B transpose. And the A transpose of with transpose should be equal to A. And, there is more properties, right? A times with B plus C, it should be very easy. What about the square matrix? That have the same number of rows and columns. We will call it about square matrix. What about the identity matrix? It means we have the ones on the main diagonal from other elements zeros. Right? So, we will call this as identity matrix of size 3 by 3.",
		"week": 2,
		"page": 20
	},
	{
		"transcript": "What about the determinant of the matrix? So, denoted by det(A). For example, for a matrix of size 2 by 2, the determinate can calculate ad minus with bc. Right? And for the larger-size matrices, the determinant of a matrix is calculated by this equation. Right? So, here, in this equation. So, Aij is a minor of the matrix obtained by removing the row and column associated with the indices i and j. What about the trace of a matrix that is the sun of all diagonal elements. Right? So, you have a sum of aii, i.e., diagonal elements. It's a trace of a matrix. A matrix for which A is equal to A transpose is called a symmetric matrix.",
		"week": 2,
		"page": 21
	},
	{
		"transcript": "The elementwise multiplication of two matrices A and B is called the elementwise product. The math notation is a circuit dot. A circuit dot with B is a11b11, basically it means each element has multiplication there.",
		"week": 2,
		"page": 22
	},
	{
		"transcript": "What about the matrix-vector products? So, you can see that matrix with the size m n, and a vector x with n real dimensions. So, the matrix can be written in terms of its row vectors. That is a1 with its transpose is the first row. Actually, you can see the a1T here. Just another kind of row vector. Right? So, that we have donated this a here, in this row. So, in each vector aiT has the size of m which means that each row will have like m elements. So, eventually, m elements. So, the matrix-vector product is the column vector of length m, whose i-th element is the dot product aiTx. For example, we want to find matrix A times with x. Basically, we will get product ax for each of their element. Right? So, notice the size of A, m by n, for x size is n by 1. Eventually, Ax has the size of m by 1.",
		"week": 2,
		"page": 23
	},
	{
		"transcript": "What about the dot product of two matrices, A and B? If they have a different size, definitely we need to make sure that this k is equal to the other one. Right? Otherwise, we will have some errors. So, eventually, we can consider the matrix-matrix product as dot-products of rows in A and columns in B. So, eventually, we can see the detail elements. And, eventually, we can find the size of n by m. The size A times with B, it's a different matrix.",
		"week": 2,
		"page": 24
	},
	{
		"transcript": "And also, we will talk about this linear dependence. For the following matrix that is B is equal to 2, 4, -1, -2. Those are set up for two columns, b1 will be 2 and 4, right, with its transpose. And b2 that is -1, -2 with its transpose. We can write b1 is equal to -2 times with b2. Right? Good? This means that the two columns are linearly dependent. If you already can write two columns, a column times with another kind of member that is a measure column. We will say these two numbers are linearly dependent. For the weighted sum that is a1, v1 plus like a2, v2 refer to as a linear combination of the vectors b1 and b2. In this case, a linear combination of the two vectors exists for which b1 plus 2 times with b2 that is equal to 0. So, that is the linear combination of the vectors. Got it? So, a collection of vectors v1, v2, to vk are linearly dependent if there exist coefficients like a1, a2, to ak that we also notice that not all equal to zero to that. Which means that the sum of ai times with vi that is 0. So, we say they are linear dependent. If there is no linear dependence, the vectors are linearly independent. Right? Any question about this linear dependent? No? Okay.",
		"week": 2,
		"page": 25
	},
	{
		"transcript": "So, now let's get started with our this lecture with today, we're gonna talk about the logistic regression model and the least squares, hopefully, we can cover all of it.",
		"week": 4,
		"page": 1
	},
	{
		"transcript": "Last week, we briefly mentioned about differences between the regression and the classification. And we talk about the simple linear regression and a multilinear regression and the polynomial regression. We're also mentioned several metrics regarding to regression problem, and the classification problem. What are the commonly used metrics for the classification problem? Can you remember? What are kind of metrics that we regularly used for the classification problem? Yeah. Confusion metrics can be one. What else? That's pull that student. Pull it back. Accuracy. Definitely, can you remember. Yes. So we also have other metrics such as f1 scores or other scores, also about ROC curve is another metric to evaluate your models. You may have found that this regression is something that we will learn today is not from the previous class.",
		"week": 4,
		"page": 2
	},
	{
		"transcript": "And, yes, I would like that we will briefly mention about classification problem again and Will try to answer the question, why not use linear regression for a classification problem? And then, we will really go to the details of logistic regression. For example, how the hypothesis presentation and some cost function, logistic regression with gradient descent, regularization. And then we will talk about how to generalize the binary classification to multi-classification. And lastly, we will talk about the least squares regression.",
		"week": 4,
		"page": 3
	},
	{
		"transcript": "The vision of classification problem again. So, typically, if it is a binary classification then we have 2 classes. Right? We'll use 0 to represent the negative class and we'll use the 1 to represent the positive class. There are several applications. For example, in your email, you may detect whether it is spam or not spam, in a medical part once we want to identify whether it is tumor, or tumor is malignant or benign, and for the transaction, we will detect whether it is fraud or no fraud, right? Yeah. Several examples about classification, binary classification problem.",
		"week": 4,
		"page": 4
	},
	{
		"transcript": "As a review about the earlier linear classification that we discussed last week, So, in this example, we have so many different lines here, and those lines are called called what? Those lines are what? What. Those lines are what? Regression? Yes. It is the regression line, but we usually call it as hyperplane or call it as decision boundary, right?. If in the left hand we choose the event in class if it is in the another side, we'll choose the class 2 here. Right?",
		"week": 4,
		"page": 5
	},
	{
		"transcript": "Why we mention a problem of it is not a good way to use linear regression for classification problem. Do you know why. Yes, That can be. That's the 1 point about here. So, actually, we more care about the details. Okay. Yeah. That can be another answer for it. But, here, we not want to use linear regression that he is a linear regression will be affected by some outliers points a lot.",
		"week": 4,
		"page": 6
	},
	{
		"transcript": "So, let's talk more details of it. So, now, give an example of tumor size, and it's my definite to see whether the tumor is the malicious or a benign. So, now, let's say, if we want to use a linear regression to fit these data points. Where it is now. Long. The line should be this way or should be this way. So this way? How should we be this way? Which way? You see him this way? Come on? Are you guys agree that? so actually so you guys need to to notice that. A linear regression, we act your own freedom that once. We want the data point that at close, not the line are close to the data point. Right? So typically, we are use this as the linear line. That it can now point to right, because it fits a lot of data points, right? And this regression lines that you guys can calculate to fit those datasets. So let's say, in this case, we can define a threshold, right, was a linear regression, and h theta x here means the output of your need. The theta here means the parameters. So let's say, for your outputs, it's bigger than 0.5, we are trying to predict it as 1. If not, we were just predicting it as 0 here, but So let's say here is a threshold 0.5. So which part is negative, and which part is positive. Is this example. Here's the threshold 0.5. Say it loudly. You mean this part is positive? Why it is positive? Do you think this part is positive or negative? It's kind of negative, right? Let's see more details. So, actually, we're using this line as kind of our decision boundary. So if it is below 0.5 you look this part, it is negative. If it is bigger than 0.5, those parts. So those 4 points, where we will say, it is positive, right. So in this example, you may think that this linear regression should be a good model, right. Can you realize the purpose of classification? Can you do that for this dataset? Is it a good one? Good one, right? Because it can successfully separate those two different classes, but However, let's see another case.",
		"week": 4,
		"page": 7
	},
	{
		"transcript": "So, this is similar as before, let's say, we have another out layer. So now for linear line, Well, it's for this guy. What will change? What will change? This line, but be close to this point right, is that true? You guys learn about linear regression, right? Alright. So now in this case, we kind of for this green line should be close to this point, right, because its boundary fit all the data points. But now, did you notice something different? Here. So let's say we still want to use the previous 0.5 as the threshold. So what will change? The answer is here. so if you would fit the best of the regression line. There still won't be enough to decide any point by which we can differentiate classes. It will, put some positive class examples into negative classes. So in this example, what 10 points will be pushed to the negative? In this case? How many points now? 3, 3 points. Are you guys agree with him? How many points will be pushed to the left? 7 point? This is four, and you will have another three right. So, actually, you will put these 3 points where previously should be in the positive class, right? So in this case, here is the decision boundary, so in this case, you will move to the negative points. So in this case, a linear regression did not do a good job. Right? This is the key reason why we won't to use the linear regression for classification problems, because it will affected by outliers a lot, right. Now, you get the point? Okay. Good.",
		"week": 4,
		"page": 8
	},
	{
		"transcript": "So, let's very talk about logistic regression. So, logistic regression is one of the basic and the popular algorithms to solve a classification problem. So, let's say, in the future, if your reviewer asks a question about what is the usage for the logistic regression. Is it for a regression problem or is it for a classification problem? If you say logistic regression if for regression probelm. Then we're done with it. But remember about that, logistic regression are really for classification problem. So that we will discuss more about it. It's known as a logistic regression, because it's underlying technique is quite the same as linear regression, and the term logistic is taken from the logit function. That is used in this method for classification. What is logit function here? You guys remember? You mean y or you mean x? Y is 0 to 1, why? Okay, so let's say, here is about logit function. So, x is 0 to 1. And Y should be any real numbers, right? Now, what you said is called the sigmoid function. So now you guys notice the differences between logit function and sigmoid function. Did you see it? So let's say what is the range of the logic function. What is x range? That's 1 in this model. Can you see it? 0 to 1, right. So what about x range in your sigmoid function? What? Your x is between what for sigmoid function. That's it. The ideal range of x is between what? Connectively even is, which is the positive you use. Right? Because you know why your sigmoid is between 0 and 1, right. Because 0 and 1 is so called the probability, are you notice that. So now you have seen the logit function is kind of opposite for sigmoid function that you guys need to remember. Yeah. Kind of yeah. There has differences between them. Okay. remember about it.",
		"week": 4,
		"page": 9
	},
	{
		"transcript": "Okay. So, now, as your review about the previous classification. So, even in binary classification, we have 1 as positive and 0 as negative. So, in that case. Let's think about the previous linear regression model. So the model is h theta x. That is equal to theta transpose times x. So basically. That's w x plus b. Right? for linear regressions. Everything you guys need to know, right? And this output can be bigger than 1, or negative 1, and even can be any number. Is that correct? Okay. Yeah. So then what about the logistic regression? So actually, we want the output is between 0 and 1. And this is very close the line of probability. Right? between 0 and 1. So previous, this tell you x can be any number. We can do what so that it can be between 0 and 1. What kind of function?",
		"week": 4,
		"page": 10
	},
	{
		"transcript": "Let's say, if we want to achieve the rule of between 0 and 1, what should we do here. Any guess. What should we do so that we can get a interval between 0 and 1. Any guess. No. It's 1 divided. It's not exactly about 1 divided. It's kind of so actually, once I point out you guys can remember, so this is really about sigmoid function. Sigmoid function by the. Actually once we apply sigmoid function, it just naturally becomes what?",
		"week": 4,
		"page": 11
	},
	{
		"transcript": "You know, like So, here's the details of the sigmoid function so, the x or z can be any number and the y between 0 and 1, and this function is so called a sigmoid function, or we call it a logistic regression. So, I think up to now, you guys know what we need to do to apply for it is a line of edition, the sigmoid function for a linear regression right so that it changes into the logistic regression. And then eventually, we have to add negative to h of x here as the 1 divided by 1 plus e minus theta T times with x. But this part, this part is really about z right the z is donated in the equation about theta T times x.",
		"week": 4,
		"page": 12
	},
	{
		"transcript": "So let's try to integrate of hypothesis output. Let's say the h of x, that is equal to the estimated probabilities, that is y, is equal to 1 on the input of point x. So, give an example, your x is equal to the transpose of the x0, and x1. What is x0? x0 is equal to what? What is x0? Any guess about x0. What is x0 here? It is kind of. But we usually called it as a bias. So the x0 here is so-called a bias. So, previous, about linear regression, we may have w x plus b, right? So, this b, the bias, so, eventually, the B, we can be incorporated with x. We can bind them together. So, that, you will find that your weighted first dimension should be like, 1. So, that is the bias. Give a, example, let's say h theta x is equal to 0.7. What does this mean? Yes. So, actually, so, that means that you can tell the patient that 70 percent chance that tumor will be malignant, right? Any questions?",
		"week": 4,
		"page": 13
	},
	{
		"transcript": "And that has more details that we will try to integrate our logistic regression. So, supposing that predicted is the y is equal to 1 once you see the x is bigger than 0.5. Why? This is z is bigger than 0 Is this correct? Why z should be bigger than 0. If y is equal to 1, What? Say it again? Yes. That key points there. Yeah. So once let's say we try to predict y is equal to 1. You have to make sure you would like to see z is bigger than 0, because you can see from the plot, right, If it is close to 1, the definitely your the z should be bigger than 0, right. If it is negative, so definitely, your z should be less than 0. Can you hear me? Good. Previous we mentioned about lots of about these theta, right?",
		"week": 4,
		"page": 14
	},
	{
		"transcript": "So, let's review this function again. So, given a bunch functions of x, xi to x capital i, so we have another kind of weights. w1, wi to w captital i here. And for the linear regression, guys, also notice that you really have like a sum of all the weight that passes plus b as a bias here. So that we can get our kind of GZ, and z is what? Z is just the sigmoid function, right, So after that, you can get a this. What's this? Yeah, this is the probability. What is the w and b here is what? Yes. And we also call them as what? Together, we'll call this as what. Parameters, theta, right? So, exactly this w and b is our theta here, right? This is the weight and bias together just the set of theta. The p just about the probability of x, in the category c1, right.",
		"week": 4,
		"page": 15
	},
	{
		"transcript": "So, let's try to see more about the decision boundaries. So, let's say, for example, the h x is equal to g theta 0 plus theta 1 times x1, plus theta 2 times x2. So, here is more examples. Let's say if theta 0 is equal to negative 3, and the theta 1 is equal to 1, theta 2 is equal to 1 here, and that we try to predict y is equal to 1 given this kind of what, this is what. This function is more like what what kind of function? Is it like a line or circle or what? This is what. This is what. It's a line, right, actually. So, this is actually this line that we brought out here, right? So you this is bigger than 0 it should be the positive. Right? If it less than, then it should right.",
		"week": 4,
		"page": 16
	},
	{
		"transcript": "Here are other examples. So, let's say if the h x is equal to g theta 0 plus the theta 1 times x1 plus the theta 2 times x2, Then plus the theta 3 x1 square and then plus the theta 4 x2 with the square here, here's more detail and different parameters of this. So then, there should be if we apply to those different numbers to it, what will we get? What will we get here? Get what? As what. Okay. Yes. Eventually, you will get like those, and then this is so-called circle, right? You got circle here, right? Good. And then, our point is just positive and negative inside is the negative 1. By measuring. And the further, we have extended the results. As you see that there can be more, like, from theta 0 to theta 1, to theta 6 blah blah, eventually, right. So, maybe we can further have kind of function in those, and then we can predict some kind of point like this. Similarly.",
		"week": 4,
		"page": 17
	},
	{
		"transcript": "But in this case, you may have question about why is this logistic regression have so, many different kind of parameters. Right? Where does this form come from? So here is the overall representations of a logistic regression hypothesis. And we have this like theta 0 plus theta 1, blah blah blah is to the theta n xn here, right? So you're wanting this problem. How we can get this kind of function here? So let's try to consider learning from our function f. That is mapping from x to y, and we are x should be a vector of real-valued features from x1 to xn, and And Y here is a boolean function is, it is just 0 or it is just 1. So, assume all xi are conditionally independent given y, and the model, P xi depends on giving y is equal to y k, as a Gaussian function, denoted in this one, right, and we also try to model P y, the probability of y has Bernoulli function as pi, who are using P y is equal to pie. Right? So, let's say what's going on about the probability of Y given x1 to xn. What, because the previous, as I mentioned, that P y is equal to pi, right, let's say, it's a Bernoulli functions.",
		"week": 4,
		"page": 18
	},
	{
		"transcript": "So, first of all, let's try to apply the Bayes rule. So, here is about probability of y is equal to 1 given x. So, apply the Bayes rule, what should be the probability here. Or should it be the probability of y is equal to 1, and then times with the probability of x, giving y is equal to 1, right. Is it correct? Can I apply the Bayes rule for this one? Let me use. So, what is the Bayes rule of this one? Anyone want to write the equation for us. Ayush is that you? Go ahead. Write for us. You don't know. Can you remember bayes rule? Maybe you can have it a try. The Bayes rule about this one. Just apply the Bayes rule. And It doesn't matter. Yeah. You can just write. What are you think about the Bayes rule in your mind, regarding this one. We got not crazy about this just about, right. Is this correct? Do you agree with him? No. It should be 1. Can you write for us? The first one? So, we are more likely to use the first one which is so called the Bayes rule for you guys. For those you can't remember it, read again in our previous slide. So, this is the something we will use. So, actually, let's try to compare with those 2 again. So, this is the P x for probability of x given y just this which the denominator is correct, so, what about the denominator? Here is just p x. So, p x is about the probability of x. A probability of x should combined all the probability of y in the 0 and the probability of 1, right. So that is can be subplace in this case. Right? 1 and 0. Right? Right? On this equation. That's not it. So then, next step we need to try to divide by probability of y is equal to 1, and probability of x, given y is equal to 1, we can get this kind of equation, right, when we try to divide it by this one? We get this case right. So, next, we will try to apply the exponential and also about the log n. What does this mean? So, you will get what? So, let's say if the value of x here is equal to 1, the same, right, same as x. So, then we'll try to just apply this case, right, so we have this exponential and the logarithm, part here. So next, we try to separate here, right, with this. What we'll do? That is this part. So then that's the reason why this part the first part, we have said this is the pi, and why this is pi? It's from probability. Right? And then we try to apply all the cases about all I in this case. Right? We still have this model here. Eventually, we will try to, tried to use the probability of x given y, so, that is really about Gaussian function. Right? So, I will not explain too much here. But anyway, so, eventually, those party just means here. And then this party is so-called theta I that we want to mention, and this is eventually why we can get a lot of status here. Right?",
		"week": 4,
		"page": 19
	},
	{
		"transcript": "Can you get it? Okay.",
		"week": 4,
		"page": 20
	},
	{
		"transcript": "Okay. That's good. Let's try to talk about the next part. It's about cost function of a logistic regression.",
		"week": 4,
		"page": 21
	},
	{
		"transcript": "So, let's say, if we have a training set with examples, x1 to x2 to xm. And here's x should be is x0 to xn again. x0 is what? Bias again, right? And there's a y, should it be either 0 or 1. And the h theta x is just about this sigmoid function. Right? So, this should be times theta transpose x here. So, that case is, how can we choose the parameters, theta here?",
		"week": 4,
		"page": 22
	},
	{
		"transcript": "As track ready, it talked about, it's a previous cost function for the linear regressions. Remember in our linear regression, basically, we try to define cost function, that is this is what? H theta x y is what? This is what? I mean, so, everything from h theta xi. What is this? From linear regression. So, it's output. It is the output of the linear regression. And is this the yi, is ground truth originally there? And this is the sum so that we can calculate the cost. And we have to make it to cost as this equation, so, eventually we can say the cost of h theta x y should be equal to one over 2 and then bra bra of this. This is from linear regression. Right?",
		"week": 4,
		"page": 23
	},
	{
		"transcript": "What about the logistic regression here? So, actually, this is the function that we defined, but the logistic regression. So, if y is equal to one so, we will use negative log h theta x. If y is equal to 0, that will use negative log 1 minus h theta x here. Let's say, in this case, if the cost is equal to 0, if y is equal to 1, you see that it's equal to 1. Is this correct? For example, even let's say h x is is close to 0 if we close to here. Now the cost should be infinite, right? So how can you draw this kind of plot here? It should be this way or should be this way? Any guess about this line. How this line changes here? In this part. h theta x is closer to 0, is closer to 1, and this part is close to what? How to draw this function here. Yeah. Here are two guesses for you guys. This is the first 1, but this 1, second 1 should be in this way. Which might be that the first one or second one? This line or this line. Why you choose? Why? So which 1 is this line? First 1. Are you guys agree with him? Yes, confused. Confused it. Okay. Why confused here. That's say if y is equal to 1, the cost is equal to 0. To be here, right? This is 1. This is y prediction. But this is a prediction, prediction is equal to 1. So, that cost is close to 0. So, then you have 1 point here, but, oppositely, if h of x is close to 0, so that costs to be given it. You have these 2 points. Right? So, then naturally should be the first 1. But is that right? But you have these 2 key points. 1 point here, 1 point here, should be like this. Right. And that's that. So, this is the plot of the cost function here.",
		"week": 4,
		"page": 24
	},
	{
		"transcript": "--No Audio--",
		"week": 4,
		"page": 25
	},
	{
		"transcript": "Can you hear me now? Oh, yes. So, let's say, if we we try to combine them together, so, eventually, we will form the overall function our cost function, for the logistic regression, as you can see here. So, why why should we do it this way? So, this is first part, why should we time with one. This is the second part. Why? We need to use the one minus y here. And guess, Why do we want to use use this one minus? Cause this is we want to match any positive number. Right? After applying this you should get positive. Right? So eventually, it will give you reduced cost function, right? We should use the negative. That's the reason why we use one minus to be here, so, eventually form the overall cost function. So, let's say the y is equal to 1. This the equation is equal to what. Y is 1. This is part. But you got it by. So, you should use negative log h theta x. If the y is equal to 0, so, that definitely is the second part. Right? So, we will get the negative log one minus theta x. So, it should be the same as the previous one. Remember, we have this negative here. Right? So, the negative the front.",
		"week": 4,
		"page": 26
	},
	{
		"transcript": "But here is the overall, that's just a regression for this cost functions. So, eventually, we tried to plug in. Right? Let's try to plug in with m examples. So, you should see the xi yi. So, eventually the cost should be previous step we mentioned. But this part, this part, this part, you see. Alright. By over m, then the sum from i to m, we mentioned that about previous for those functions. Right? And this is the case that you guys need to learn all kinds of about logistic regression. Eventually, for example in our gradient descent, we are really trying to minimize those functions. Let's say, during learning you are trying to find a parameter theta, and that is just minimized j theta here, j here, then generate prediction, given new x, generate the output of h theta x. This is logistic regression that we try to predict the probability of it. Yeah. I mean, questions here. What about online students? Are you guys have any questions so far? Okay. No. Let's continue.",
		"week": 4,
		"page": 27
	},
	{
		"transcript": "So, let's talk more about where does these cost functions come from another way that's to try to use likelihood function to see it. How can I get this? Similar as before, we have many examples from x1, y1 to xm ym. The maximum likelihood estimation for the parameter theta. So, definitely should be MLE here. So, we try to maximize the parameter for probability from x1 y1 to xm ym here. And they it can be donated with this part. Right? Continue the product from i equal 1 to the m. Now you go right? And then as far as our maximum condition for the maximum likelihood estimation for the parameter theta here.",
		"week": 4,
		"page": 28
	},
	{
		"transcript": "So, let's say our goal is what. Our goal is try to choose the parameter theta to maximize the likelihood of training data. And this is like the probability of y, which y equal to one given the x, and this is probability of y equal to 0 given the x here. And this is just typically h theta x. And this should be 1 minus h theta x. That's you guys get it. What about our data model, likelihood? You should be also like to continue the product from i to 1 to the m. About all the parameters of p theta here, all the points. And this is the conditional likelihood that is of probability of yi given the probability of xi. Right? And here is the overall stuff that we tried to optimize. That is the maximum conditional likelihood that this is about here.",
		"week": 4,
		"page": 29
	},
	{
		"transcript": "So, as for how we can get the closed-form solution we were trying to discuss more about it, expressing the conditional log-likelihood. What about this? Once we take the log of the product. What can we get? Once we try to take the log all the product. What can we get? This is what? This is what? You know, you will have the sum. Is that correct? Right? So that's why the reason why this part changed to this one. Are you guys get it? And further, we can donate to this previous is the extension of all about the probability of yi given xi here, so, it's just final equation about everything here, and can think about the first this part is before to what. Let's say why y is equal to here. Any guess, any reasons? Why the probability of y given x is equal to here? You should notice what about the y. The y, we have how many we just have 0 and 1. That's the reason why you can expand this should be 0 or what. This part is what? And eventually, I can get another version of it, so, eventually is this equation. Great. And this is similar as before. Once we try to get a cost function y is equal to one. You should be, like negative log h theta of x. If y equals to 0. So, we can get the negative log of 1 minus h theta x.",
		"week": 4,
		"page": 30
	},
	{
		"transcript": "Next, the point that we try to discuss about the logistic regression using gradient descent method, how we can get approach this kind of parameter theta here.",
		"week": 4,
		"page": 31
	},
	{
		"transcript": "Or is the j theta here is what. J is what here? Yeah. J is the cost function, also called the objective function because we define and our goal is what? Just try to minimize J theta here. What about good news? Good news is that it is a convex function. At a convex function, this what, You can find a minimum number of it. But the bad news is what. There's not analytical solution. We have to use the gradient descent to approximate this number. Right? So, that is remember this is the key points about gradient descent. Right? We have the theta j, and the theta j minus alpha. That is the directive of j theta that respects to each each theta j here. Right? What about alpha here? What is alpha here. Yes. That's correct. Alpha is learning rate. So, then next step you're trying to think of updates, all the parameters of theta. So, here, this is about how we calculate these derivatives, all these equations.",
		"week": 4,
		"page": 32
	},
	{
		"transcript": "So, let's continue about it. Once we try to plug in. It should become this one. Right? Okay. You're mine. The slow. It's kind of like snow. It's really about the learning rate. How fast it is? Eventually, should it be just the about this part. Right? You see this? That's the reason. We've already check the derivitive respected to the theta J here that becomes here. But that's the key reason why the change in another way. Right? That's good point.",
		"week": 4,
		"page": 33
	},
	{
		"transcript": "Let's try to remember, what is the gradient descent for linear regression. This is the previous for the linear regression we get. And here's now, we have a logistic regression here. Any difference in your mind? No no difference? What? This part? that part? so this and that part is similar. Right? What I'm thinking is that, what is the h theta x. H theta x is the same for linear regression and logistic regression? Are they same here? What is the h theta of x for the linear regression? Why is it equal to what say again? It's a linear line. It's a mathematics, so, you say. It is more like w x plus b, that is the h theta x. What about the h theta in logistic regression? Can you remember, we just mentioned? Exactly. That is the sigmoid function. So, that's the reason the key difference is in h theta x. H theta function previously, it's just about linear equation, but in this case, it is really about a sigmoid function. So, in this case, the teacher can pause people to watch. That Yes. So, theta t, so it combines both the weight and the bias here. Right? So, that would be called the theta T. But always remember, in the first dimension, we have bias. And this is function for the logistic regression. So, this is the key difference between linear regression and the logistic regression during the gradient descent. In which part?. Alpha. Yes. That's a good point. So, actually this part is allready there.",
		"week": 4,
		"page": 34
	},
	{
		"transcript": "Next part we will talk about the regularization. Next we wanna discuss about regularization here.",
		"week": 4,
		"page": 35
	},
	{
		"transcript": "Previously, we mentioned about the maximum conditional likelihood estimate like MCLE and also mention And now, what about MAP? What is MAP? The max. What is MAP? Any guess, about MAP is what. It is what? It is the maximum prior about MAP. Right? And this is the case in that is how the maximum conditional a posterior estimate. MCAP and this is how we donate in this case. What are the difference between them, the only difference is about in the prior. What is the prior? The P theta is prior here. In the next slide, we will mention more details about P theta. This is very about guassian functions. This P theta is really about prior. Only difference between MCLE and MCAP here.",
		"week": 4,
		"page": 36
	},
	{
		"transcript": "And here is the common choice of P theta that we usually try by the normal distribution will be the identity covariance, and a pushes parameters towards the zeros. And here are different kinds of regularizations, and it can help, it can help avoid a very large weight and overfitting. That's the one reason, why we want to use MAT. Because we very try to avoid by weight and overfitting. So, yeah, the case about all different kinds of regularization that you can see here.",
		"week": 4,
		"page": 37
	},
	{
		"transcript": "Then eventually, we we try to see a difference about the gradient descent. Right? So, first part, you guys already noticed that this is about maximum conditional likelihood estimate, MCLE results, and here is the results about the MCAP. So, the only difference is that and another kind of parameter, not parameter, is a penalty here, alpha, lambda, theta j here, and lambda here is a kind of parameter. So, this is the key difference that once you introduce the part here, so, you eventually will get a kind of penalty once you try to use gradient descent. And then this is the difference between MLE and MAP here. Maybe we can take a 5 minutes break, we can refresh your memory. We have lots of math here. Right? Do you think it's a math instead of machine learning? By the way, we are going to machine learning part. This is the key theory. You really need to know the math behind it. Try to understand those, you are easy to implement code. Right? Cause the code will base on it. You guys will see it. Okay. That's try to have 5 minutes break.",
		"week": 4,
		"page": 38
	},
	{
		"transcript": "Maybe we can get started. So, actually, up to now, I would like to say for the majority about the logistic regression you should have best common about everything up to now. So, the next one I want to discuss about the multi-class classification, because previously we just talked about the binary classification problem. So, what about the multi-class classification we try to use logistic regression method.",
		"week": 4,
		"page": 39
	},
	{
		"transcript": "Here are more examples, for example, email holder or tagging, you may have multiple tagging for example, emails from work. Emails, from friends, from faculty, from Hobby, etcetera, right? What about medical diagrams? So, some sometimes you are not ill, you get a cold or maybe you catch flu. Right? About the weather, it is sunny, it's like cloudy, it's rainy, it's snow, it is all different classes. For different multi-class classifications, right?",
		"week": 4,
		"page": 40
	},
	{
		"transcript": "What about the difference between the binary classification and the multi-class classification? So, from the plot, it's just very intuitive. Right? So, because in a multi-class classification, we have more than 2, that's it, right?",
		"week": 4,
		"page": 41
	},
	{
		"transcript": "So, how about the logistic regression in the multi-class classification? So, we so-call one versus all, or one versus all the rest. So, this is about 3 classes, right. So, it's in this case, for the logistic regression, we usually perform as assuming it as a binary classification problem. What we do here is for example, for the h theta x, we're trying this class, this is the class one right, this is blue, the real class. All the rest, we plan as another class, right? So, what about h theta 2 here? So, we actually care about the. X 2 here, this is class 2, and this is all the rest. What about the h theta 3 here, we just care about the real numbers, and others so-called all the rest, right. And this is so-called one versus all here. And this is the equation that we try to pay attention to this multi-class problem that h theta i x. I is what? Classes. Yes. And this is the best about the probability of y equal to i, that a given x, and parameter theta here. Right? So, this is actually we're trying to pay attention h theta i x here.",
		"week": 4,
		"page": 42
	},
	{
		"transcript": "To train a logistic regression classifier h theta i x for each class i to predict the probability that y equals to i. So, let's say, given a new input x. We will try to pick the class i, then try to maximize i in the h theta i x. This is the best maximum probability, right? This part is very about the probability of that. Is that correct? So, then you guys need to remember what about the for the multi-class that is try to maximize the probability of each class, the corresponding class, right? This is about multi-class that you guys need to remember.",
		"week": 4,
		"page": 43
	},
	{
		"transcript": "What about some methods comparison, the generative method and the discriminative approach, the naive Bayes. So, let's try to estimate the probability of y and the probability of x given y, and this is about prediction. About the logistic regression, where we are trying to estimate the probability of P y over x. Right? Also, called the discriminant function. So, then we will study about the SVM later, but you guys need to remember for the logistic regression, we really try to predict this probability y over x. Right?",
		"week": 4,
		"page": 44
	},
	{
		"transcript": "And then here are some more readings guys, try to look at the details of it.",
		"week": 4,
		"page": 45
	},
	{
		"transcript": "And the next step, we're willing to try to let you guys to remember what we learned from this class so far. So, we have learned about the hypothesis representation, this is very about a sigmoid function, and we learn about a cost function of the logistic regression so, this is really about this, right? For the y, if y is equal to 1, it should be negative log h theta of x and y is equal to 0, they should be negative log one minus h theta x here. And what about the gradient descent? You guys need to remember we use after alpha should be 1 over m this sum h theta x minus y then times with x j i here. That's about the regularization. Remember with the additional additional what. For the addition what we can get it. Right. Yes. For the addition prior, so, that we can get this kind of equation. The alpha, lambda, theta j is really about another penalty here. So, lastly, we learned about the multi-class classification, for using logistic regression that is try to maximize the probability in the corresponding class. As you see, the key point from this lecture about the logistic regression.",
		"week": 4,
		"page": 46
	},
	{
		"transcript": "Next, let's try to explain about the details. How do you guys need to implement the logistic regression from scratch with Python? So, previously you guys just call function from scikit learn. But in this case, you guys need to implement by yourself. And here are the details that you guys can look for. So, first of all, it's about standardization. So, this mu is about what. It's about the normalization of the data. Right? This on is easy. The next thing is initializing of your parameters. You will have a feature vector that is size m by n and plus one. Why it plus one? All is a feature vector, you have m times n plus 1. Why is this plus 1? And also, you have weights, kind of weights, or theta here, n plus 1 times 1. Why is n plus 1? This is the ambient number of points, and the n here is the number of observations or number of features in each point, right, so, why is n plus 1? Yes. So, that is a bias. So, you guys need to remember once you try to implement it, you should make sure the dimensionality. I'll say you have n plus 1. Keep in mind. And next, you needed to implement the sigmoid functions should be very easy. Right? But, this is the thing that you guys need to remember that is about the dot product of this feature vector, and the weight here. Next step, you guys need to define a function of the cost function. So, this is the cost function that you guys need to define in your homework. Next step is really about the gradient descent. Right? The gradient descent, this is the key part, right?",
		"week": 4,
		"page": 47
	},
	{
		"transcript": "Let's see about the kind of gradient descent. Stochastic gradient descent that you guys familiar with. Let's try to quickly overview about it. Right? We have L. We have loss functions. We have f. We have the function parameterized by theta here. We have x and y, training inputs, and y is the output. And then this is the key part why how can you finish the gradient descent? Is really about repeat. The number of iterations that you guys need to define. Right? And for each xi, probably, you can skip this part, but you need to definitely calculate what is the estimate output. You'll have a function f, right? h theta x need to calculate about output that is y hat. Y hat is the prediction of your. Y hat is the prediction of what? Of your logistic regression model, right? And after that, you can calculate a loss. The loss will tell you how far of y hat i and output here, right? So, next, you really, you need to use this how you should move the theta to maximize the loss. And see how it updates eventually, you use theta minus, eta, and the eta is equal to alpha, as the slides. Just a learning rate. Another parameter guys need to define previously right? And eventually, that is about gradient descent. That's you guys needed to use in your homework.",
		"week": 4,
		"page": 48
	},
	{
		"transcript": "So, for previously, the gradient descent that you guys already know what? In the gradient descent, what will you learn here? What are the key things that you guys can learn here? In the gradient descent. Parameters. The parameter just about what, what kind of parameter? What? What? It just the weight and bias that you can learn, right. And this should be the 6 steps that you guys need to do the homework about the logistic regression model. So, then we will try to review it again about the logistic regression. To be honest, it's not always this, not always this. You can it's better if you use this, but sometimes you have a very strange dataset. You need to standardize the dataset. But it is not necessary for this part. This is something I want you guys to keep in mind. Given in the future, you guys need to use normalization of your data. That's good. Yeah. Any questions from our online students. Oh, no.",
		"week": 4,
		"page": 49
	},
	{
		"transcript": "And the after that the 6 step is about the prediction. How you can make a prediction? So, once you get weight and bias from the gradient descent that you guys can really make predictions in any new points, right? That you can get a y hat. So, that after some steps you can calculate accuracy. This is just about what does this mean. Those part means what? Yes, this is the total number of points. The total number of points you already predict. Right? And then divide by the length of y and get accuracy of your model. Right? So, now after now, We've finished about our logistic regression part. Any questions so far?",
		"week": 4,
		"page": 50
	},
	{
		"transcript": "So, then we will talk about the we will quickly talk about the sklearn really about the multi-class classification is a logistic regression. So, this is key things that we try to load about the dataset from sklearn. Then we try to load the linear model from to the logistic regression then we try to use the dataset to make prediction. So, this is just a few lines, guys, Now about sklearn is very easy to handle logistic regression. Right? But, you know, another case, during an interview your future could be, the future reviewer might ask you to implement logistic regression from scratch. This can be one interview question for you guys. Which means that during you guys' homework, you guys need to pay attention to it not try to google it. Because for you, you need to remember about all the content in the slides. Try to think about carefully by yourself, instead of google the answers. It's not good for you guys. So, anyway, if you guys have any questions, let me know about your limitation of it.",
		"week": 4,
		"page": 51
	},
	{
		"transcript": "Let's try to go to our another section about this lecture. That is very about the least squares regression that you guys probably notice about it. Is that true? But anyway, we just have a few slides that try to review. Regularization? In your mind do you think it is useful? Not sure. So, yeah. That is the key good question. So, for that regularization, we're really to want to avoid over-fitting problem. For regularization, you will add another penalty, right? In gradient descent part, you will make it smoother and avoid to give over-fitting of it. That's the one advantage. And another thing is about the dataset, the dataset. If you just have a few datasets, it will be very easy to decrease gradient descent. So, in that case, the regularization should be another option. Previously, we talk about the logistic regression about classification problem. Right? Now, let's talk about the least squares regression. It's not in this case. It's very about regression problem.",
		"week": 4,
		"page": 52
	},
	{
		"transcript": "So, giving a dataset that x or y to xn and yn where x. Should be x and y are not random. The Yi is equal to alpha plus beta xi plus Ui are random variables from i to n. The random variables from Ui to Un have zero expectation and variance sigma square here. So, the method of least squares that try to choose for alpha and beta, let's try to satisfy this equation. That means What's this? This is what? It's really about least error right, but this part is about yi minus alpha. It's called what? No matter how, we have a few slides to discuss with the details of it. But now you guys need to remember, this is the overall part that we need to optimize for our least squares regression. Right?",
		"week": 4,
		"page": 53
	},
	{
		"transcript": "So, next, the observed value yi corresponding to xi and the value alpha plus beta xi on the regression line that is y is equal to alpha plus beta x here. So, typically, this just a fitting line, right? And this is just 1 point, but you have another point, and this is the point from the regression line here. Right? And here is the object functions that want to achieve the least square estimation. Is that clear about this function?",
		"week": 4,
		"page": 54
	},
	{
		"transcript": "So, next, we very want to discuss how we can estimate those parameters. For the least square, we will choose a value for alpha and beta such that this kind of function is minimal. You'll find the least square's estimates we differentiate S. Alpha beta with respect to alpha and beta respectively. Right? That we will set the derivatives is equal to 0. Is it the correct way? okay. So then, if let's say, let's check that derivitive with alpha with respect to here. Take the directive about S with with respect to alpha. What can we get here? Take a piece of paper out. You need to find equations now. You have paper in hand you can try to calculate. If you do not have a paper we have blackboard. Who do not have paper here? Oh, Now, who want to do it for us? This part should be very easy. Right? Should I call name? Raise your hand. Go ahead, do it for us. Please respect to alpha. Just respect to alpha. This one. This S. Square also. Square. Yes. This is square. This is sigma i to n, yi minus alpha minus beta xi and then square. That's it. You miss this one. So this is the overall S here then you check the derivative with alpha. Any of you already have answers? No. No answer? You have answer? Do you write down? No? Okay. No worries. You you should be first. Let's do what's going on. Yeah. It's fine. Yes. And also, do you have answer? Just a respect to alpha. Okay. Let's see. Do you have answer? No. It doesn't matter. There has 2 here. Right? It has the square here. Right? Once you have this kind of problem, you probably will move this 2 to be here in the front. Then you will copy this, right, then do a derivative with this to alpha again. Right? Yeah, move two to the front, then copy this one here. Yes. Yes. Continue. Yes. This is the 2. Yeah. Yeah. Copy it again. Copy this full equation, and then you will time with another derivative that is respect to alpha. Yeah. So, now alpha should be negative 1, right, so, you'll can pull negative 1 in the front. Right? Yeah. it should be no. but the xi it should put there. Right? Yeah, the combination of xi was those right. It's about But it doesn't matter. I understand your part. Yeah. Go ahead. Yeah. It's fine. You guys get an answer? Let me check if you guys have. Oh, yes. How about you? Do you have answer here? Seems correct. Now let's say more details about alpha here. So, first, this is just very easy about this. This one, once you try to take respective with alpha, you will put the 2 in this front right? You can see you put the 2 to the front then you copy of this equation. So, then, the second one  you check the derivative in the inside is negative one, right? So, you will have negative 2 here. So, now, because we need to set it to 0, so here the negative 2 is gone. Right? This is really about the first one. Right? Does that make sense? That part? In the second part? Let's really do it closely. So the key things is what? The key thing is about the square here. The 2 here. Right? The first one, your first part you should get this 2. That is fine. sigma from i to n then you will have yi minus alpha then minus beta xi here. This is your first part, right. The second part will take derivative respect to alpha. The inside should be negative. Is respective to the alpha to be negative 1. Once you try to set them into zero, you can cancel this negative 2. This is the key thing that eventually we have this part. What about the respective to. Let's try to expand this first. What is the answer for this one? Once you try to expand those. What will you get? Show us, what't the answer for this one? Try to expand this. What can you get in this equation equal to zero? Can you come to write for us? For this one, what can you get? Now you really notice that although you guys have the kind of background. Once when you calculate, kind of confusing, right? And this the key things and very normal machine learning course and practice. This one? Can you pull the n to here? Yeah, make it better. Yes. Yes. Is it correct? This one is right. Let's look into details. The detail is n alpha here. Why you have n? The sum of alpha. So you will have n alpha here. Good right, do you get it? What about the second one? The second one. What is the difference about the second one? Second one is what? Write for us for the second one. Which is with respect to the beta. What is the answer for the beta? It is just times with sigma of xi. Get it? No matter you can do it. No matter how, you can do it for us. Where is the alpha come from? This is two, sorry. Sorry it's two, okay. So, now, next you need to take it respective to the beta. Yes. What is the answer for this part? To the beta? This is zero. Yes. What about this part? This should be just xi. We will keep the summation here. So, eventually, should be just xi in the right. Right. You guys understand it? Okay, good. Go back, so, now, this finally get this one. The only difference is that the additional xi here that is equal to zero. So once you try to expand those one, you will get this equation, right? Next with the most important part for you guys is how to solve this alpha and beta here.",
		"week": 4,
		"page": 55
	},
	{
		"transcript": "This is key thing again. You guys have two minutes to solve this kind of problem. Pinxue, can you show for us? This one, probably solves alpha and beta. You need to solve beta first. So then after that, cause once you get beta, alpha should be very easy here. Go ahead, come. Come to solve this alpha and beta. Write down, I will check each of you to get something solution. You don't need to cancel alpha. For example, if you want to calculate the beta, you need to cancel this alpha. Right? To cancel the alpha out, For the first 1, what do you need to do? You first need to divide by n. So, then cancel this part, right? So, then you can cancel this alpha alright. The first equation divided by n, then times with this part. Go ahead. Can move them together. Most of this n to be there. move the n to this part and this part. Just one over n here, one over n one over n right here. Okay. So, now go ahead. Then times with this part. No. This equation, you still need to times with. You can check that, but it's fine. You can write eventually about that part. You actually can do that. But now, you need to calculate that first then. Is it easy? It's not that easy as you saw right? Finish. This beta? You finish. Is this correct? How can you get this one? Is this correct? Can you explain to us what's this? This part? Why you remove, This is what? This is this part? What's this? Explain to us. How can you get this equation? Okay. So, let's try to verify whether this correct or not. Okay. So, this is about really about alpha, and compare. Is this correct? Can try to pull this n out. Should be times with n here times with it here. There should be equal. Right? Is this correct? Yes. What about if we have beta here. How can you get alpha? Is it easy to get alpha? For example, this case, there should be alpha just equal to what we move just this part then divided by n. Right? Is that correct? So, then become just becomes here, right? That's correct. This part? You missed a square, right. There's no square. It should be another square to be here. So, here about the details of it.",
		"week": 4,
		"page": 56
	},
	{
		"transcript": "So, this is, again, about alpha and beta here. All the negative in the middle, you probably have this part, this part just about the covariance x y, and this part about variance of of xi. So, eventually, we are trying to get about this covariance xi divided by variance xi. And also, this equation is equal to this one , x y bar, and x bar comes with y bar, and divided by the x square hat of mean, and mean of x with the square. Right? And eventually is this. If you guys do not believe, if you have time, you can try to verify whether this equation is equal to this one. Try to test your knowledge whether this one is equal to this one. Okay. It's kind of homework. Let's continue with it. Once you get the alpha and beta. Check this equation. Those are 3 key points. Try to write down what it is alpha and beta here. Calculate this with respect to these 3 points, right, 1, 2, 3, 1.8, 5, 1. What is alpha and beta? If you calculate, just let me know what's your answer for your beta here. What those 3 points again? Did anyone get the beta here? Beta is what? Can you write down here. Write down here. For online students, do you get some results about beta and alpha? Not yet. So, n should be just equal to 3. Are you guys get the same results? Or you have different answer. Let's say if this is the exam during the course, I give you guys this kind of calculate. How long do I need to calculate this equation? There still have answer from any of you? Do you get the answer? You can write down your answer here. Yeah, that doesn't matter. Okay, this is your answer. What about other students? Did you get the similar result? Here? Juju, did you get some result now? Yeah. I I got beta have 9 point 22492452499999. Nah,Nah,Nah,Nah,Nah,Yeah. 25. What about your alpha? Alpha not yet. 1 second. I see you guys probably get the different answer. Opposite, oh you write in opposite. It doesn't matter. What about the rest of you guys? Did you calculate the alpha? Alpha is 2.3499999. Is that right? What about others? Still no answer yet? Let's say, if it is an exam you guys already take too many minutes to calculate this equation. Right? The step is hard? Okay. Now let's try and see whether you guys have a good number for it.",
		"week": 4,
		"page": 57
	},
	{
		"transcript": "It should be very easy for you guys to calculate another one. As I said before, what's this? This is really we need to talk about the residual. So, this is a way to try to explore whether the linear regression model is appropriate to a model given our bivariate dataset is to inspect a scatter plot of the so-called residuals ri against the xi here. So, the i-th residual ri is defined as a vertical distance between the i-th point and the estimated regression line. So for example, this should be yi minus alpha hat minus beta hat xi. Why it has that hat here? What does this hat mean for this kind of? It should be estimated parameters. So, that we had hat here. And this is so-called the residual that we can get from the model. But always you needed to remember that sum of all residues should be equal to 0. Is that true?",
		"week": 4,
		"page": 58
	},
	{
		"transcript": "This can be a quick test of your knowledge about this least square estimation. Can you get the alpha and beta a little bit? Let's try let me try to write down this. What should be 1, 2, 3, 1.8, that we have y 1 and 3 points here. What is the alpha and beta? That's give you several seconds for our online students. So for this is 3 points. Can write down the 3 points here, and then try to estimate the alpha and beta. So, the 3 points are these 3 points. 1, 2, 3, 1.8, and the 5, 1. Do you see these 3 points? Do you see it? Yes. Got it. Let's try to go back to this equation. So, the answer is what alpha should be, no the beta should be neGative 0.25. It should be good. And this one should be 2.35. Right? Now, this is the number. You guys can remember, right. Always try to be patient. Even for this, it's so simple calculation. Most student can do it, you see, you still can get something wrong. Right? So, always keep in mind. Be careful with everything. As you look for the rest, it should be very easy for you guys to calculate about the residual. I will just skip this part. You guys can try to calculate once you have time.",
		"week": 4,
		"page": 59
	},
	{
		"transcript": "Let's try to continue. So, that should be everything from today. You guys think about and really about to implement the logistic regression model. And then once you implement your model, you need to compare results from the function, from the sklearn, and they try let me try to quickly go through the homework with you guys.",
		"week": 4,
		"page": 60
	},
	{
		"transcript": "",
		"week": 4,
		"page": 61
	},







	{
		"transcript": "This slide is talking about this is a machine learning course and today is lecture 9 which talks about k nearest neighbor algorithm, also called knn. the professor is Youshan Zhang. the date is march 15th, 2023.  So then, let's try to discuss this lecture. So today, we're going to talk about knn. So, actually, if you want, talk about the knn. I don't know whether you guys know about the use of Chatgpt is it just released the version 4 yesterday night.  can any one of you who has that or tries to have a version or do these systems experience? No. Yes or no. That's it. That's important. It's totally the same, but it is an upgrade. So previously, it was chatgpt version of 3.5 turbo.  This time, they have upgraded to the version 4. You should have a better capacity, you know, is make a prediction, or Make other. And also, even force for the image as an input for you. So previously, it can only input text.  But this time, you really compelte with the multimodel data sets. So, actually, you can just input the images. In the demo, the issue that a bunch of which are kind of amazing, they can quickly write a website for me to give you my   image like that way.   And also, if you can find some strange document cannot picture the 1 that can find some strange voices during the picture.  To say which part is, not correct, not correct. Alright? I think you probably let's try to play some fun thing here. I'll try to integrate your homework and chatgpt.",
		"week": 9,
		"page": 1
	},
	{
		"transcript": "This slide is talking about a challenge, which is if student can use chatgpt to modify their code and don't detect by professor, they will get reward, otherwise they will lose points.  Okay. This is about Thanks. It's those 2 students. They should have come. So here is by the rule that I want you guys time with a little bit of challenge with you with your homework. Let's say, how do you code it is.  How have you try it before. do you try that? No? I tried using to debug. Using it to debug?. Yes. it can debug for you. what about others, did you try that? while don't all of you? No. And add it to commands.  The add commands. yeah, it can. So now, actually, as this time, you probably are having more of more time to try with chatgpt. You see, once the ability of you and the chatgpt which one is better.  Now, from now, I guess, we only have 5 typically weeks left. Right? Remember we have another 2 weeks for holiday. I guess you guys will have another 5 different weekly homework, like you would give a few previous weeks.  Right? I want to like to invite randomly 5 students, and you guys can decide yourself, and do not let me know who is that person.  Okay? To try to challenge it. So second one, we'll want you. You can feel free to modify your code to chatgpt. To see whether I can catch you or not? Is that a problem? Doesn't it make sense?  If I catch you, you will lost 2 points. If I cannot catch you, you will get an extra 5 points, and the other one student, I guess. You will also get a 5 extra points. Does that make sense? Do you want?  So now it's ready your time to gain extra your points. If you find if you think some of your homework is not that good. Now it's good opportunity to try it. any questions for here? Yes or no. No. You completely understand the rule?  do You wanna try it? You want it? Yes or no. No. not one challenge yourself. So this time you can get your extra points here. Anyone who do you want to? And you can decide by yourself later,  but  I want someone I want I need a volunteer. you'll help me to collect 5 students. And you guys can decide who will play the first. Once I graded it, for example, but today's homework, I will grade in the next week.  Right? Once I finish, then you can let me know who is that student with chatgpt. If I completely did not detect you, you can get an extra 5 points.  Doesn't make sense. Yes or no? ruslan. What's your idea about it? That's interesting. Interesting. You want it. So later how many of you guys decide by yourself? And maybe who can collect that name for me.  Correct the name for me. You want to? Yeah. Okay. You can ask the roommates. Oh, well, no roommates, is your classmates. Who wants to participate in this how to say, part of . But you will feel free to modify the code.  In a way that I cannot detect you. Right? That should be good away to do that. And do not let me know the person, unless I have ready to give you grades. You can let me know. Which one is that person use chatgpt.  Why I want to do this? Because I think, for a majority of you is just in order to check it. Typically, I think this is the very fighting tool, and it is like huge generation of the AI.  And I don't know whether a guy heard about the AICG. You heard about the AICG, What is the AICG here? It's called unofficial intelligence for our content generations of the AICG.  So this is actually A new generation, once the chatGPT becomes so popular, Right? Once the chatgpt releases online into the text less than several days, that I think one trillion users, which is a crazy number.  And you guys, maybe you need to play with it, to see whether it can save your time? But, the key thing is that you need to use it as a tool. You'll have yourself understand that the questions.  Especially when sometimes has seen the nowadays. It's grammar. You feel very good job when you're writing some challenges of that work. Yeah. But try it. You can get some good result, obviously, I'll hit.  Okay. Is that clear? Well, everybody What is did they use that at your PMT? If you did not use it,  you just do normally as your assignments, submit it normally. But So, the wrong students, I guess, you will still get a 5 voice.  Okay. But as you see, definitely, you guys will need someone, 1 students, only 1 students in the per assignment. I do not need all of you participate in.  Okay. So for example, in today's homework, only one of you is submitted with chatgpt. Okay. This is whether I can detect you or not. Because this is also very important to us.  Because of our faculty because our computer science may be a little bit challenged, but of course, some arts major, like some history major many some students use it to write some CCs and the plus and per visit, they can't really   cannot detect it, which should cause a disaster for them.  Now, I really want to challenge myself to see whether I can detect, if you use that or not. But Again, if you use this, like, put away, they're saying you can completey understand code.  They're also primary, because now now they codes are online everywhere. People are just copy you here. copy there. It's just more like the collection of code. No matter how, right?  It's just a 1 way question. We will try to Google a problem, it'll give you some solution, and then a solution also. I believe some of  you who might be using some code from online, right, just to give us a similar way.  But the policy say that chatgpt they can output some most likely correct answer. But you need it to your pay attention to that, Sometimes it can give you some wrong answer.  So you need to double verify above it. Okay. ",
		"week": 9,
		"page": 2
	},
	{
		"transcript": "So, now let's talk about with today's lecture. Remember in the 2 weeks before we talk about k means, which depends on them, k means depends on what? Yes. it is purely depends on the distance between dataset.  Right? k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. ",
		"week": 9,
		"page": 3
	},
	{
		"transcript": "And the last week we talk about the EM algorithm for ocean picture model. But, you need to use an e step, and m step to ask out dates.  So many the parameters, you mentioned it, you still can get different classes of it. But what is difference between k means and em algorithm? Mhmm.  Yes. Yes. That's correct. Because k means is just a hard assignment.  But for the k means, not k means, but a GMM model. It is the softer level of algorithms that, and you need to base on probability. You get some posterior and you get some probability of your dataset.  So this is the key thing, the key difference between K means, and there's the EM, GMM model. ",
		"week": 9,
		"page": 4
	},
	{
		"transcript": "So, into this lecture, we will be able to tell still talk about a method that is close to the distance based.  It's called a knn model. First, we're gonna try to mention different learning measure. And after that, we just talk about we're going to talk about the knn model, because knn model is not that complex.  And lastly, I will talk about the advantages and the disadvantages of the knn. So first of all, let's try to view that given the different learning measurements. ",
		"week": 9,
		"page": 5
	},
	{
		"transcript": "The first part is called eager learning.  It means explicit description of target function on the whole dataset. What does this mean? This means what? So accurately, this means that you need to define, for example, functional, specific or function f.  Right? You need to know exactly about the function of apps so that you can get a training. But for instance, that's the learning. So, actually, the learning means that you try to store all training instances here.  And the classification is what? It's assigned a type of function to a new instance. And these are also referred as a lazy learning. In today's model knn, based on your learning, is which kind of method.  Knn is an eager learning, or is instance based learning? The second one? Okay. So Remember my answers. So that  you will know whether tehe second one is correct or not. for instance-based learning, also called lazy learning. the   learning part is storing all training data, and the classification part is assigning target function to a new instance.",
		"week": 9,
		"page": 6
	},
	{
		"transcript": "That's no to the next one. For the eager learning, so we try to describe more examples. For example, this human, it can be a bunch of balls here, So next, you will try to see random movement is like a mouse.  You will try to see as this function. So now you can tell you saw a mouse. Right? Now to try to play it again.  If we were to detect a random movement you will know, this random movement is a function that tries to define this behavior. Right? So this is about like a mouse. I saw a mouse with this person can learn about it.",
		"week": 9,
		"page": 7
	},
	{
		"transcript": "And for the instance based learning, for example, you still have, and this is kind of movement. You'll give this a bunch of examples, like a descriptor like phones, and even about a desktop.  What you need to do is that you try to calculate the distance between the detected item with existing ones so that you will tell which one it is most like.  So now it's very similar to that stuff. So in this case, you will classify this man and what? That's what? It's more like a desktop. Why? Because it is close to one for instance. And this is so-called the instance based learning. As you either need, you compare the data, one data set of always or other instance.  The classification of machine learning algorithms into lazy and eager learning categories stems from their fundamental differences in handling and processing data. The key distinction between lazy and eager learning in machine   learning lies in if and when they generalize from training data.  The latter does that during training, whereas the former avoids deriving general rules or builds local models for each object it\u2019s asked to classify.",
		"week": 9,
		"page": 8
	},
	{
		"transcript": "Here. Has what about the instance based learning? The idea is that in trying to combine the similar examples, have a similar level and classify a new example, like a similar training examples, the algorithm is what?  So, give you some new example x. For which we need to predict as class of y, find the most similar training examples that try to classify x.  classify y also more than similar examples. You got it? Well, that is obviously Or does it mean? This means in these straight lines, you means you will do what? Predict class. Yes. So, exactly, it's just a busy, then we'll mention the right because you have given some new example x. Okay, this is new example x.  Right? To find the most similar training examples, you need to find the most similar one, that was sort of established. Classify x like those are most similar So you will classify this one as a most similar desktop.  Right? That's it. So now we have another 3 questions. First of the question, yes, how to define a similarity? Any idea how to define similarity?  Yes, distance. Tell me one with the other layer to categorize similarity.  Okay. What about others, how do you determine the similarity? We have another similarity. And now if you remember, you guys will talk discussed how we calculate that   similarity.  Yes. there should be another frequently with 1 that we use.  The second question is about how many similar training examples to consider, how many? Do you know, in this case? All you need is to define.  This is actually how I say anything. You probably need to define how many Similar examples you want to consider, right? The last question is about how to resolve that inconsistency among are training examples.  So later, we are seeing more about the answers of these questions.    the idea of instance-based learning is that similar examples has similar label and Classify new examples like similar training examples.",
		"week": 9,
		"page": 9
	},
	{
		"transcript": "In today's lectures, actually, we will talk about the first one the knn, that is answer your question previously.  Right? The knn is one of instance based methods. And also, there are other couple Of models like regular regression and has the best reasoning.  So later, we will not talk too much about Rest two, majority of today's lecture, we will focus on the first. ",
		"week": 9,
		"page": 10
	},
	{
		"transcript": "This slide is talking about Nearest Neighbor Classifiers,  the basic idea is If it walks like a duck, quacks like a duck, then it\u2019s probably a duck.  The best idea is simple as before. Let's say, if it works like a duck, the purpose like a duck, then it is probably a duck.  Let's say this example. This is actually your training report. You have this different animals. Right? And this is the task of the course. So in this case, what's your answer? Go ahead, your prediction. Let's say you want to predict it. So you will see this like what. It's like what? It's like this this one, I like this one. You don't know that. firstly, you need to calculate distance.  Right? Once you calculate distance, you try to find the minimum, and then you will find that this is the more likely The duck, it should be the nearest one. But actually, from the real world is   the nature of duck. Is not right. It's just a swimming pool, which is not in those three. But this is the only way, or you want to use KDS number this is the only level you can get, because those are most similar examples, right?  And this is another drawback of the knn model.",
		"week": 9,
		"page": 11
	},
	{
		"transcript": "The feature of knn are All instances correspond to points in an n-dimensional Euclidean space, Classification is delayed till a new instance arrives ,Classification done by comparing feature vectors of the different points , Target function may be discrete or real-valued , k-NN classification rule is to assign to a test sample the majority.  category label of its k nearest training samples , In practice, k is usually chosen to be odd, so as to avoid ties. And then here are some key features. All instance correspond to points is an n dimensional euclidean space, and the classification is delayed until a new instance arrives.   And then here are some key features. All instance correspond to points is an n dimensional euclidean space, and the classification is delayed until a new instance arrives.  The classification done by comparing feature vectors of the different points, the target function may be displayed a real value.  But knn classification rule is to assign a test sample in the majority category level of its k nearest Training examples, actually, in practice that, the k, is usually chose to be odd.  So as to avoid ties, right? You need to pick the top k levels. Everything is odd, it should be easy to generate.  if it is an even numbers sometimes it will be confusing.",
		"week": 9,
		"page": 12
	},
	{
		"transcript": "As the official definition of the K nearest neighbor,  1 nearest neighbor spot. You will get the nearby only one example. And if two nearest neighbor, for examples.  You got an active 1, another positive 1. And for 3 nearest neighbor, you will have another 3 examples. So in this case, will get a 1 negative, and you will get the 2 positive, in this case.   which means that k nearest neighbor of a record x are  data point sthat have the k smallest distance to x. So in this case, if you use 3 nearest neighbor, what is the prediction? For this x. x is postive. What about one nearest neighbor? negative. what about the second one? it can be negative or postive.  because you care about two nearest neighbor, you don't know which 1 is better.  Yeah. If you choose a smaller distance, that can be one solution, but if you choose it to be positve, it can also be correct. Right. So that's key reason the k should be odd.",
		"week": 9,
		"page": 13
	},
	{
		"transcript": "So, for our giving record to be classified, identify nearby records. \u201cNear\u201d means records with similar predictor values X1, X2, ... Xp.  Classify the record as whatever the predominant class is among the nearby records (the \u201cneighbors\u201d)",
		"week": 9,
		"page": 14
	},
	{
		"transcript": "And this is answer for our previous question how to measure the nearby or how to measure a similarity.  So, actually, for the knn model, the most popular distance measure is about the euclidean distance. The best measurement of the distance of the 2 points like that.  Right? Get a severe growth of it. ",
		"week": 9,
		"page": 15
	},
	{
		"transcript": "And this is about how to choose the k. K is the number of nearby neighbors to be used to classify the new record.K=1 means use the single nearest record. K=5 means use the 5 nearest records. Typically choose that value of k which has lowest error rate in validation data.   So, typically, choose the value of k, which has the lowest error rate In the validation data set. This is one way of how to choose optimal k here. What is the lowest of error rate?  How would happen like this error rate or the so called an error rate? Maybe you could try several several colleagues and features in condition is the error rate. So how would you find this error rate?  We'll be trying to several It's not exactly. So, actually, for here, the error rate you can think about error. We calculated, so now  if you'll give us a specific k, we will get output of that k.  Right? So you need to calculate the distance between each and different levels so that you can get a number, if you choose another k, and you can get a second number.  So in that case, you will determine which one is smaller, so that when this is minimum, we'll choose that the optimal k.  Okay.",
		"week": 9,
		"page": 16
	},
	{
		"transcript": "So, here's comparison between low k and High k. So low values are 1 or 3, it'll capture a local structure in the data.  But then you may have some noisy. But if you get a high value of k, it provide more smoothing, less noise, but may miss local structure, the extreme case of k = n (i.e., the entire data set) is the same as the \u201cna\u00efve rule\u201d (classify all records according to majority class)",
		"week": 9,
		"page": 17
	},
	{
		"transcript": "1-Nearest Neighbor is One of the simplest of all machine learning classifiers its Simple idea is labelling a new point the same as the closest known pointLabel it red. So, what is the outcome for this 1?  red.  Because it is close to red. ",
		"week": 9,
		"page": 18
	},
	{
		"transcript": "So, if k equals 3,  So you will have 2 reds or and you'll have 1 blue ball. So eventually, you should be just a red. Right? If let's say, if the k is 7, we can calculate all 7 points. So, in this case, How many groups here? No idea. k equals 7, but you have blue. Right? because there are 4 blue balls in there, and red ball only have 3. So, actually, you have found that if you change your k. So sometimes your results can be different.",
		"week": 9,
		"page": 19
	},
	{
		"transcript": "Right? This is An arbitrary instance is represented by (a1(x), a2(x), a3(x),.., an(x)) ,a(x) denotes features. Euclidean distance between two instances, which is  d(xi, x j)=sqrt (sum for r=1 to n (ar(x i) - ar(x j)) 2) when target function is  Continuous valued, get the mean value of the k nearest training examples.  here's the training examples. What does this mean?  It is a regression problem, but so let's say, if you give regression problem. the k is 3, what is what is the label for this 1? What is the label with this 1? If it is a regression problem. Yes, exactly.  just calculate the mean, the average.  Right? Remember, for knn, it can apply both for classification also if it can apply for regression model.",
		"week": 9,
		"page": 20
	},
	{
		"transcript": "So here is about all other different distance and metrics. For example, you have an minkowsky distance. This 1, you have the euclidean space that we already discussed. And also, you have manhattan distance.  And others, you have quadratic, and correlation, the chi square, the kendall's rank correlation, They have so many different distance function.  So sometimes, if you choose 1 of them, you still might get a little bit different of results.  Again, you can use these in knn. ",
		"week": 9,
		"page": 21
	},
	{
		"transcript": "So, next up, I want to make a mention about the Voronoi Diagram. So this is about a Decision surface formed by the training examples .  So suppose you have so many different examples. You try to plot out. and now you may remember.  Okay. That's fine. But the Voronoi Diagram try to find a kind of polygon, for some lines, you know, different examples. So, here is an example. The diagram separate different examples.",
		"week": 9,
		"page": 22
	},
	{
		"transcript": "And here is a more details about that. We have 2 important properties. The first 1 is that All possible points within a sample's Voronoi cell are the nearest neighboring points for that sample.  Which means that, given any 2 of them, you see, there will be a line Between them, and those 2 should have been nearest neighbor.  This is for example, this are 2. This also should have been  nearest neighbor Right? And also, those 2 should have be nearest neighbor.  So every points like a pair can find in this diagram right now. Yes never. And the second property is For any sample, the nearest sample is determined by the closest Voronoi cell edge.  You should be you find this page, the nearest should be this 1.  Right? It's just determined by this Voronoi background. So this Voronoi background help your guys find the nearest neighbor. ",
		"week": 9,
		"page": 23
	},
	{
		"transcript": "So, now let's try to play an example of knn for similarity, matrix is a number of matching attributes So in this case, let's say the k is equal to 2, let's say if we have a new example, for example, the first 1 For the food, you have great, you have a chat, you get a no, and for the fast, you get a no.  And then for the price, you got a normal, and then for bar, you got a no. So, eventually, can you predict, what is this?  You person give you a big tip or not? What is the answer? Yes. Why? it is the most similar. Is that correct?   So, now, which is the closest example in this case.  Well, which are the 2 closest. in this case. So I wanna Mhmm. Yes. K is equal to 2.  Their numbers, once, and 2, so you will get an answer of you. Yes. Probably this person will give you a big tip. Right? So you identify the most similar sort of number 2, and another 1 just in this 1. So you eventually will get a yes.  What about another example here? So this This person we got is this mediocre, you got a yes, and then you got a no, and then you got a normal, you got a no here. Which one for this?   So this 1 says, no. And what is the most closest 1?  Yeah. Is That correct? The other closest would be this 1?  So now in this case, what's your answer So, actually, this this 1 is correct, the most similar one. And the second 1 says, yes. So eventually,  There can be either yes or no. so you need a good k here.",
		"week": 9,
		"page": 24
	},
	{
		"transcript": "So, here, let's try to talk about selecting the number of neighbors. So if we we try to increase the k, it will Makes KNN less sensitive to noise. But when you decrease the k here,  it Allows capturing finer structure of space.  we shouldn't have pick a k that is too big. But also, we do not we should  not pick a k that is too small.  In this case, It depends on our dataset. So typically, when we define k as 3 or 5, like those kind of numbers.   is there a way to compare the total data points to the number of classes given on them. Okay? I'd say there's only 2 classes, and there's a hundred data.  So,I I think I think the more classes, the smaller k is , and the more data, the bigger k is . That's a general idea, but I don't know if you're more specific further than that.   We'll have an idea regarding his question. And then we just mentioned how we could choose that kind of ultimate k. So in this case, it depends how on dataset.  We probably need you to calculate the error rates of your nearest 5 or data points. once you get a k, that's that we can try to    calculate that distance between the class to the points. Right? You will choose which k give you minimum number.  Okay? Does that make sense?      As I said, this is kind of quick conclusion, you shouldn't choose a big or small k.",
		"week": 9,
		"page": 25
	},
	{
		"transcript": "Here, I want to briefly mention about Distance-Weighted Nearest Neighbor Algorithm. So this means that you will try to Assign weights to the neighbors based on their \u2018distance\u2019 from the query point.  And Weight \u2018may\u2019 be inverse square of the distances and all training points may influence a particular instance. And this is the so called the shepard's method. So let's see, I'll have an example about this.",
		"week": 9,
		"page": 26
	},
	{
		"transcript": "So, if you try Using weighted Euclidean or Mahalanobis distance can sometimes help. for example, if you have this mu minus and mu positive, you have these 2 different classes. Let's say, here we add new example here. We probably need to calculate their distance to here, calculate our distance to here, Other than that, you probably want to assign a little bit of weight to mu 1 or mu 2.  So in this case, you want to use a smaller WI, which was a horizontal axis of feature in this example, because this 1 actually is more close to this example.  Right? The lesson previous you think about is, as the green button, eventually, we probably need to update it as the blue here.  So notice that Mahalanobis distance has all the effects of detecting the access which you have. For example, in this case, you can try to rotate it over your previous horizontal.  So now you can move a little bit to this angle. So eventually, you can add a new canal for coordinate.  You're going to find some move.  You'll get a move of the coordinated. So this time, we actually just have 2 by 2 symmetric or metrics. In this case, chosen by us or learned it is similar like the comprehensive metrics that you guys previous notice.  And then you want to find that by using this weight, you have found that you move most that closing points close to each other.  Why? You see it? Now, as a distant curiosity, it is very long run. While you are applying for this waiting, they were trying to model the center but method them close to each other.  And this is key thing that we want to the weighted distance here. ",
		"week": 9,
		"page": 27
	},
	{
		"transcript": "There are several remarks.  Highly effective inductive inference method for noisy training data and complex target functions. Target function for a whole space may be described as a combination of less complex local approximations. Learning is very simple.  Classification is time consuming.   Why it says that the classification is time consuming? Yes. Exactly. for knn, it is an instance base learning. That's the measure. You need to calculate that distance to 1 point, to all others. Right? So that's the reason why it's a little bit of time consuming.",
		"week": 9,
		"page": 28
	},
	{
		"transcript": "And then here is about the knn time complexity. So Suppose there are m instances and n features in the dataset Nearest neighbor algorithm requires computing m distances. Each distance computation involves scanning through each feature value. Running time complexity is proportional to m \u00d7 n.   Let's say if n is very big, because eventually it will be very big for Time complexity. Right?",
		"week": 9,
		"page": 29
	},
	{
		"transcript": "And then we will have the 7 problem as before about curse of dimensionality, that is a Prediction accuracy can quickly degrade when number of attributes grows, which means that if we have too many irrelevant attributes, we can easily swap information from relevant attributes.  So when many, irrelevant attributes, the similarity or distance measure becomes less reliable, Right. Because you have too many noisy points in your dataset.  So the remedy you'll be able to try to remove that you irrelevant features in preprocessed steps. And weight the attributes differently, and also tried to increase k, but it cannot increase too much.  in Your mind, there was another way you try to to, like,  reduce the curse of dimensionality. Do you have an idea that never before?  How do you handle this because of the dimensionality? Say again. SVD. Right? But the svd is just for the decomposition of metrics. Yes. Actually, Yes, because you will need to use PCA to reduce dimensionality.  That's correct.",
		"week": 9,
		"page": 30
	},
	{
		"transcript": "Let's try and quickly talk about advantages and the disadvantages of knn.",
		"week": 9,
		"page": 31
	},
	{
		"transcript": "So, the advantage is that it is very simple to implement an algorithm.  And there's no assumption required about the normal distribution blah blah, and it is Effective at capturing complex interactions among variables without having to define a statistical model.  And it requires a little tuning and Often performs quite well!  you can Try it first on a new learning problem.",
		"week": 9,
		"page": 32
	},
	{
		"transcript": "And definitely, there are many other disadvantages.  knn Need distance/similarity measure and attributes that \u201cmatch\u201d target function. This can be a large data sets. Right? As we've just discussed about its time complexity. It is m by n. Both of them are mine and it can be a disaster for large dataset. so, Must make a pass through the entire dataset for each classification. This can be prohibitive for large data sets.  And  Prediction accuracy can quickly degrade when number of attributes grows. because of \u201ccurse of dimensionality\u201d.",
		"week": 9,
		"page": 33
	},
	{
		"transcript": "And after that, there are several ways that we will be dealing with the curse of dimensionally, first one is PCA that it has been mentioned.  And the another one is that Computational shortcuts that settle for \u201calmost nearest neighbors\u201d (adjustments in XLMiner settings).   What about another way, if we use dimensionality. Remember, yeah, you said pca, what is other ways? If you have so many different attributes or features, what is another way?  And remember? Let's say if i give you1 side of dimensions, pictures, where is the most critical way to make it smaller. Use the features Selection.  Yes. feature selection. You choose some of the features, right, that's it. ",
		"week": 9,
		"page": 34
	},
	{
		"transcript": "And here's a quick summary about today. so we try to Find distance between record-to-be-classified and all other records. And next, we try to Select k-nearest records. Classify it according to majority vote of nearest neighbors Or, for prediction, take the as average of the nearest neighbors.  for \u201cCurse of dimensionality\u201d , it needs to limit of predictors. ",
		"week": 9,
		"page": 35
	},
	{
		"transcript": "So, probably we can take around about 10 minutes break. So I will talk about pseudocode.  which is the most important part for knn.  Let's get back. So did you guys finish the next 1?  Okay. Got a 5 students. Good. Let's see what's going on. It's gonna happen to 5 weeks. Probably remember, you need to have keep your copy of original chatgpt code.  Right? So that you will know how many modification you make, and also I will know, if I had made some mistake, right? I will know why I made those kind of mistakes. And maybe you guys have learned enough.  Let's see what's going on with it. Right? Well,but homework, you just need to submit your own works. So later, if I ask you, your send me the chatgpt version. Again. Okay. Is that here? Yeah. Good.  This is should be I guess about final slide. But today, we're gonna talk about pseudocode . The first step is that you Load the training and test data, then  Choose the value of K .  For each point in test data, find the Euclidean distance to all training data points,  then the Euclidean distances in a list and sort it ,  then choose the first k points ,    then assign a class to the test point based on the majority of classes present in the chosen points  Let's say, if you you already have the y of the training data set. Right? Like, trend y, So that's by using the majority of the levels, a majority of the level in the this 1 so that you can decide which class it is.   Okay. Good. So this is the  pseudocode part of that algorithm.  It's not that complex. ",
		"week": 9,
		"page": 36
	},
	{
		"transcript": "So, here is the homework, you guys, and you needed to implement the knn. You only follow the code that I've put online.  You only need to follow those up. It should be this 1. Only follow those. So it basically just has 3 steps. Okay. Let's try if you see about the  homework.   So first 1 is about the implemented knn methods. And next, they just load our give you our train and text matfile, and then you need to perform the KNn, and then report the accuracy of the test dataset.  The last 1, will try to reduce the dimensionality, use PCA to low dimensions. For example, you can try to reduce it to 10 or 3, depends on you. Now then, get the accuracy of test data again, to see if there is a difference  Okay? So now you have some time to play with this homework. I will upload it now. You can open a laptop. Try to do that.   This can is 1 of the interview questions. They gave the students around 20 minutes to finish it. Have you ever challenge yourself whether you can finished this 1? I mean, the first that's the first 1 and 2 in 30 minutes.   Kindly challenge yourself. Okay? Is that clear?",
		"week": 9,
		"page": 37
	},
	{
		"transcript": "",
		"week": 9,
		"page": 38
	},




	{
        "transcript": "",
        "week": 11,
        "page": 1
    },
    {
        "transcript": "And in this lecture, we're gonna talk about another traditional method for the SVM, support vector machine.  So here is the overall overview. I will quickly mention about history of SVM, and then talk about some properties of SVM eventually will be discussed. about this simple applications use SVM models.",
        "week": 11,
        "page": 2
    },
    {
        "transcript": "So, first, the SVM model is related to statistical linear theory. And the SVM was first introduced in 1992, later as SVM becomes popular because of its success in handwritten digits recognition, So, actually, they got a 1.1 percent test error rate using SVM, and this is the same as the error rate as a careful constructed neural network LeNet 4.  Remember, we talk about the net in the deep learning course. Right? So that The SVM actually had shared a similar kind of performance as the net.  That's my reason why SVM become so popular at that time. So now, SVM is regarded as an important example of a kernel master. and one of the key area in machine learning.  So, usually, as SVM model, we are getting kind of better performance than the models we discussed in the last few weeks like this KNN model, decision tree, and among them, SVM should usually and get a better result. But then we ask you how depends on your tell us that.",
        "week": 11,
        "page": 3
    },
    {
        "transcript": "We quickly review about this linear classifiers Let's see, if you use this black dot represent the positive one and we use this circle to donate the negative one.     How can you classify this data? Okay. Last one is. Sure. Do you write it in the right time? Yes. You guys thinking about you want to draw a line about it.  Right? For example, this can be one line that can distinguish between positive one and negative one Let's suppose that this line is w x plus b. That is equal to 0. w is what? weight. Yeah. It is the one weight. you can call it the slope of this line.  what about b here? Intercept or the bias of it. Right? And in the left side, this is the bigger than 0, and in the right side it should be less than 0.  So do you notice there is a function that can classify them into 3 different domains that is bigger than 0 or equal to 0 or less than 0. They are such a function. You can do that.  Is there a function f that can convert w x plus b to specified different categories?  sigmoid function. sigmoid function. Simple function. sigmoid function, typically, what is the range of the sigmoid function?   The sigmoid function properly like real numbers. Right? It wouldn't just give you three different classes. Right? So here is something that you guys probably heard.  What about sign function, you remember this function? the sign function once you get it as f, w x plus b, so than you can get a predict of y. what is the plot of sign function? Do you remember?  It should be like this one. Right? If it is a negative 1 or 0 or the positive 1, which is corresponding to these 3 different categories. Right? If it is equal to 0, then it is 0. If it is bigger than zero that should be 0. If it is less than 0, it should be just the 1. Right?   It were three different states you can think about why we want to use the sign function of it.",
        "week": 11,
        "page": 4
    },
    {
        "transcript": "This can be one possible classifier line. Right?",
        "week": 11,
        "page": 5
    },
    {
        "transcript": "This can be another one.",
        "week": 11,
        "page": 6
    },
    {
        "transcript": "That can be multiple of those lines. So now here's the question. Any of those would be fine but which one is the best. Which line is best. Can you tell? The line with some condition of these lines. Yeah. That's kind of so.",
        "week": 11,
        "page": 7
    },
    {
        "transcript": "Actually, there's lots of lines between them. Right? Let's see if a point is close to those that can be misclassified to the positive one.  Right? So, actually, it is close to the that one. But in this case, because the because of this line, it is misclassified and positive 1, and you you choose another line. So accuracy should be correct.  Right? They're also done. Let me keep the line. So in this case, you might need it to thing about that, you need to find and have good lives so that you can make sure you can class correctly classify these different classes.",
        "week": 11,
        "page": 8
    },
    {
        "transcript": "But, actually, that can be a stick line between different classes.  Right? So during the SVM problem, we actually define a margin so called have a linear classifier as a white that the boundary could be increased before hitting the point.  Right? So before is left or right before you're shooting the classes, you actually can draw a margin between them.  As you can see in this orange in orange in margin in this case. So, actually, you can hit as a boundary. So that part is the h is a blank, and it's a white one.  It h is a circle. the white circle here. Right? So this is so so called a margin.",
        "week": 11,
        "page": 9
    },
    {
        "transcript": "So, go above. The SVM impeller is actually finding a maximum margin linear classifier that is linear classifier with the maximum margin here.  And this is the simplest kind of SVM, so called the linear SVM, or called an LSVM. And in the Sklearn package that you guys can use this linear SVM to do a classification.  But now let's try to look at the details of those so called max margin. So in here, in this blue line is so called maximum margin. As you can see, it is very thick.  So what about these three points? What are them? They are on the boundary of the module. So what is the specific names of this kind of points? Any idea here. Actually, this is the boundary point is to called the support vectors.   And those support vectors just means that are those data points that a margin portion are up against each other. So in the future, your interview will ask of the similar question, like, what is the support vectors?  What are the support vectors? They are on the margin. Yes. You should answer that. So Support vectors are those data points that are on the margin of it.  Right? This can be some easy question for you guys. You need it, w, you need it to remember. What is support vectors. Okay? And there are 3 different observations. First one, maximizing the margin is good according to intuition because you can directly separate it is 2 different classes. Right? And then then getting in bytes that only as part of vectors are important in the same all those three points may be important than the right.  You you will really care about those margin points. Other training points can be enrolled. Right? because as long as you can find those kind of support vectors, you can determine your best margin.  And in properly, it works very, very well. If we have find as long as we can find those kind of support vectors, your probably can get the very good accuracy.",
        "week": 11,
        "page": 10
    },
    {
        "transcript": "So, now let's talk about details of how we can get the maximum margin. we will starting with how to calculate a distance from a point to a line. And remember, there should be your high school course. Right?  calculate one point to line. For example, that's one line, is that point here, how can you calculate the distance between one point to a line. Yes. And remember? Like projection draw a line perpendicular to the line and calculate the distance.  That is one solution. So can you remember as a specific ingredient of it? Have right of us. Try the best way. So write down your line here. Let other people know what is your line.  Otherwise, what is your line. The highest one is the what is it? Address on the point. and AB is AB is just the. Mhmm. Write down your answer, you're all. So it should be plus b. Right? b y plus c 0 is that your line?  Yeah.  So you think this ABC just this couple of abc right? Right? What is the small part of it? Small abc is what? Smaller, Oh, actually, you change the smaller to the captical?  what about others? can you remember another one?   So what about Pinxue, did you get some results?  You remember what is the line distance?  No? Don't remember?   who can remember this equation of ditance, from point to line?  It seems you guys are all of you guys are forgot about it. Never mind. So we will review a quick review about that. It is not a very big point.  But anyway, in this case, we typically talk about if we have x, We have x1, x2, so now because it is 2 dimensions. So once we plus in with it, we have w1 plus x1 plus w2 times with x2 plus b is equal to 0 because it is like 2 dimensions.  So, eventually, your w is w 1w2x. So the x1, x2, so it costs bound to one point. So eventually, it can be to a 0.",
        "week": 11,
        "page": 11
    },
    {
        "transcript": "So, here is about the distance equation compared from one point x to this line.  Is that correct? It should be this one. Yes. Is it similar to this one? Did it similar to here or not. Similar? You can only change it is to cap to smaller.  Right? that's no significant difference between this one and this one. Okay?. What about others? can you remember this line?  Is this one correct? It should be just a normal. The wi just means the each dimension of the w.  Right? You have bunch of it. You should have been w 1, w 2. Like, previously, you have 2 dimensions. Right? So you should get the sum of it. and questions is here. Okay. So let's continue.",
        "week": 11,
        "page": 12
    },
    {
        "transcript": "So, now we really talk about we have defined the margin. Okay. Let's continue. To talk about maxmium value of the linear SVM about its maximum margin. So let's take an example of it. So remember, this is a line of w x plus b is equal to 0. Right? So if you think the predict class is equal to negative 1, so you get another line. Right? That line is equal to w x plus b that is equal to  negative 1.  And then the other line has a w x plus b that it just equal to 1. So now you have given the 3 lines. So now we already know this is 3 different styles. So what about this case? How we can get this equation.  It is third equation. How can I get this one? Yes. Use the first one to subtract the second one, so you can get another equation here. Right? So next, this is how we try to define the margin.  So actually the margin, it is the width of the margin. Right? Then you find  this equition that is x positive minus x negative. That time with w then divided w. So eventually, we'll get the equation of 2 divided by the norm of w.  How can we get this from this module here?  Just use the first and second equation to replace for the third equation.   So from this event, again, this is like this. Right? So this part is just a right here. Right? So and, also, you can expand this 1, like, with the x positive time with w is equal to what, that minus this is why it's equal to what.  In the next slides, we will try to discuss it, again, how we can know this is just the line, the margin.",
        "week": 11,
        "page": 13
    },
    {
        "transcript": "Right? So this is that we already noticed that we have a we can use this. This is the previous we can move this one to this part.  That is w times x plus minus b is equal to 1, and the w times x minus b is equal to negative 1. And according to this projections or you just use this kind of math.  Right? So, actually, this is the margin that we can calculate. So best of it, this is how we define the margin of it, the y's is equal to x plus minus x minus times with w divided by w norm.  What does this mean? This party means what? W divided by w norm. It's just a a unit vector, right?  Right? because at this point about which direction, right, So, eventually, we once we have the unit vector, he has just this part, and we expand it, we can get one minus b minus negative one minus  b minus divided by norm w  So in the future, if you Some other people ask me another question on how to define the margin of SVM.  You should immediate notice that you can get. 2 divided by norm of the w. w is what?  The slope. Right? That is just 2 divided by the norm of the w. So w here is not just a simple slope.  Right? You should also you think it's a vector. You shouldn't be out of stock. Right? But, anyway, you can think about what w is slope, but eventually, this shouldn't be just the norm of bunch of points there.    So it should be eventually w is a vector. Right?",
        "week": 11,
        "page": 14
    },
    {
        "transcript": "So, next, we wanna talk about how we can correctly classify all the training examples. As we already noticed that what is w, x i plus p is what?  This part is y. It's just y. Right? So you will have to classify whether y i is equal to positive one or this negative one. So these are the 3 different examples.  Once you try to combine together, How can I get this 1? Is this correct? Is this correct?   Okay. I got it. So you had just single line. y i in this part? Why are you using this negative ones you could view?  Changes the direction of this or less or equal. Right? Once it comes with if it let's say, for the first one, it should be easy. Right? But most this bar aggregate definitely should be Big or equal to 1.  But for a second case, let's say even y i is equal to negative 1. So what's problem in here because this is negative you should change this condition be bigger or equal to 1.  Right? So, eventually, it can get for the all i, it it shouldn't satisfy this equation. Right? Right? Good. So, remember, I could just talk about the max margin, you should be having it equal to 2 divided by m.  Right? Do you sorry. Divided by w. or it is similar as that is one over two, w transpose time with w. Is that correct? It should be quite equal to each other.  That's a significant difference between them. And then we will find another problem, so called the quadratic optimization problem. And the in that case that we tried to solve. for the w and the how can we get this kind of numbers.  We always need to remember what this is quadratic optimization problem. We have a minimal loss function that is tried to minimize its 5 w. That is equal to 1 over 2, w transport times w. So this is the overall goal.  We want to make make sure this is as smaller as possible. And this is a subject to y i, then w x i plus b, that is bigger or equal to 0 for the whole i. here, and this is the overall problem that we want to solve yet.  So in your mind, if you have this kind of problem, how can you solve this problem for you. You will have this  optimization problem. How can you solve it? directive, you can use directives.  So what your point is directive. to solve this quadratic optimization problem. You remember, we we can do something with it.",
        "week": 11,
        "page": 15
    },
    {
        "transcript": "Okay. Let's continue to talk about some solution here. Again, here is our problem.  So this is we want to minimize this 5 for all x y to satisfy all equation. And for the quadratic optimization problems, so they actually are very well known plus our mathematical program problems.  And many of these algorithms can solve this problem. But one easy solution is about that you need to apply lagrange multiply, so that is alpha i here.  So now in this case, we also try to apply this alpha i in this equation. That is associated with every constraint is a primary problem.  So in this case, we defined the quadratic optimization problem that q i, let's say, that are some of all i that is minus 1 over 2. This should be a summation, that is y i y j x i transposed with x j, it should be maximized.  And, also, you have another constraints that is the sum of all alpha i and y i is equal to 0, and all the a i it should be bigger than zero for the each a i here.  So eventually, we convert to the previous quadratic optimization problems to this one uses this Lagrange multiplier to help us get some solutions.",
        "week": 11,
        "page": 16
    },
    {
        "transcript": "And due to real time, this time, we we are only quick to talk about solutions. So eventually, we will get w. it will equal to the sum of alpha i, y i x i transposed x i  And the b here is equal to y k minus w transposed x k or any x k such as that alpha k is not equal to 0. And then for the each alpha i not equal to 0, alpha i indicates that is our corresponding x i is our support vector.  This is another indication of it. So, once you try to solve it, You should notice that if your alpha i is nonzero means that we will find the x that is a support vector.  Okay. Then the cosine function, we have the form of FX that is equal to sum of all alpha i, y i x i transposed with x, then plus with b.  So basically, this part just right. This part just This part should be what? How does this one? This part just should be your w. Right? The first part is just your w.  And, also, remember, in your homework, you will also need to implement this as SVM algorithm. You should always remember that even sometimes you find that dimensions are not agree with each other.  Please do a transpose of it. Okay. Feel free to use the transpose to do that.   And noticed that it replies in the product between the test point x and the support vector x i, we are returning to this letter.  And also keeps in mind that solving the optimization problem with all the computing that inner products, x i, x j between all the pairs of training points.",
        "week": 11,
        "page": 17
    },
    {
        "transcript": "So let's quickly saw some examples. First, we'll talk about what is the high margin so far we we find all data point to be passed by practically.  Right? Which means that there's no training error. So what if the training setting is noisy, the solution 1, we will try to get a very powerful kernel. Is this good?  yes, you may get overfitting for your dataset. if you are in the training, you get such a good kind of classifier and then do it on the test, it definitely will not get result very high accuracy.  To resolve this kind of issues so that we define about another margin so called soft margin.  So remember, all you need to remember what is a hard margin. So hard margin that means what.",
        "week": 11,
        "page": 18
    },
    {
        "transcript": "So, for the soft margin, we can allow some misclassification. Right? because Eventually, overfitting is not our goal. Right? We want to get as high accuracy as possible for our test dataset.  So eventually, if some researchers, they propose a slight variable, epsilon i here that can be applied or added to allow this classification of difficulty of noisy examples.  So now we change it. As you can see, they are just several different epsilon here. Right? Well, those points, so that they can allow to maximize our margin.  So eventually, this should be our new objective function. Right? Remember, this part is a previous that a hard margin. And this time, it had introduced soft, a select variable, a sigma epsilon i here.  So eventually, it's from the k or small k to the r.",
        "week": 11,
        "page": 19
    },
    {
        "transcript": "So, this is our new objective function. So remember, this part is our old formula, then we end to use of this kind of the first part.  Now in this case, once we incorporate the slack variables we aim to find the w and b is such as that as phi w that is equal to this part of them plus this epsilon i here.  epsilon is what? Absolutely. Well, Another name is part of what? Very good. Ah, please share with us in this name.  So noise. I thought what is this? Regular. Regularity It's kind of. Errors, it is not just errors.  What has this the epsilon that we just mentioned? Yeah. Just in the previous slide? So now you guys already did not remember. Right? It's just the one slide before.  What is the or here. It's just the Slack variables. Right? Now you with 10 seconds, you you all of you guys forgot about what's that? This is just a slack variable. You can simulate it, like, kind of a regularization term of it. Right?  So in this case, we want to minimize this new objective function, and in this case, you should have constraines to this new conditions, and we have this epsilon i that is bigger or equal to 0 for all i and eventually you change this term from for this, this is minus, epsilon i here.  What is parameter c here? The parameter c can be viewed as a way to control overfitting between two terms. Right? So eventually, in your homework, you also need to define the c the balance is different term here.",
        "week": 11,
        "page": 20
    },
    {
        "transcript": "And then here is the overview about Linear SVM. So the classifier is separating hyperplane, and most important training points are the support vectors.  Right? And the they are defines the hyperlane the the quadratic optimization algorithms that can identify which are training examples xi are support for the vectors.  With nonzero, like along the upper i here. And the both in the dual formation of the problem or in the solution of training points appeared only in the inside dot products.  And remember this is about is that alpha i is what? What is alpha i?  Alpha i is lagrange multiplier, right, to help you solve this problem. And then, eventually, you can donate your equation as w plus b, this part is a part of w.  Right? Once you get this kind of solution, you should approach that. What do you get from it?",
        "week": 11,
        "page": 21
    },
    {
        "transcript": "So, previously, we just talked about linear SVM. What about the nonlinear SVM. So what are other differences between linear or nonlinear?  Linear is what? same line. because the model must like a line. What about nonlinear. In the multi dimension, you know it will just like a curve right? It's not a simple linear relationship.   For x and y. So previously, we talked about datasets that are linear separate with some noise with our grade, for example, in this case, if we can separate this red and blue.  So definitely, once we have our single line, we can identify this two cases.  Right? So what about we are going to do if the data set is just too hard. For example, in this case, you'll have both. red in the left and the right hand of this blue, this you may think about probably just not a simple linear model.   What is the solution if you want to solve this problem? Mhmm. Okay. So this problem. We want to classify this red and the blue dots here.  Increase the dimensions. Right? So exactly the answer is here.  you want to map the data to a higher dimensional space. So in this case, even we try to feed our dataset is at spare.  Right? So in this case, of course, this red dots and more likely another way so then you can try to classify them. Right? So in the next few slides, we will try to discuss well about the nonlinear SVM.",
        "week": 11,
        "page": 22
    },
    {
        "transcript": "we will try to project it to the high dimensional space, I think we probably reviewed it before. Right? Maybe in the early weeks, we talk about the open mobile to high dimensions.  So the general idea is that the original input space can always be mapped to some higher dimension of each space where the training set is separable.  So in the left hand if you just think about in the two dimensions, There's just a lot of, like, blue dots and red dots. How can you classify them?  The red dots and the red dots. Yeah. In this case, more likely you need to find a circle to separate them. Right. So if you let's say, if you project them, you don't hide dimensional space as this is a three dimensions.  Right? in this case, you just you need a what to separate them. Plane, Is this a plane?  So this is more like a surface. this is a surface in 3 dimensions. Right?",
        "week": 11,
        "page": 23
    },
    {
        "transcript": "And that is related to another important topic called the kernel trick. What is kernel? What does kernel mean here? Kernal means what?  So the kernel firstly means that you want to point the Data mostly be used, like, the dot product of this data points. So this is in the first line, this The linear classifer rely on dot product between the vectors.  K is so which we define as a kernel that is kernel of xi, xj that is equal to xi transpose with xj here, and this is usually how we defined kernel So in the future, you may also need to remember if someone want you define a kernel, you should remember about this way.  You should always can be written in this format with some x i and maybe a phi x i or phi x j there. and here is more details  So if the added data points is mapped to high dimensional space, we have some transformation plane, so that means you need to map x to the phi x here.  So the dot product becomes the k x i x j, that is equal to phi x i transposed phi x j.  this is the key thing. So In the future, if someone wanted you to define a function, you need to remember that. The kernal function should be written in this way.  Right, use it as transpose with another number. Okay. So now let's say, our kernal function is a function that is corresponding to in the dot product in some expanded region space.  So here is one example in the 2 dimensional vectors. as x is equal to x1, x2, and here is how you defined the kernel function.  This is the k x i  x j that is equal to 1 plus x i transposed x i with a power of 2. So let's see, this is the kernal function, you need to write a a way of phi x i transposed that it comes with pi x j here.  Right. This is the purpose of your you guys need to do. Do you wanna try it by  yourselves? Let's probably take a little bit of the break. Okay.",
        "week": 11,
        "page": 24
    },
    {
        "transcript": "After that we can take a look at this kernal function. How can you do it by yourself?  Okay. Okay. Probably we can get back. Talk about this kernel. What is the kernal like, let's start start with this example.  Take a piece paper out. Write down whether you can get this kernel, in a format of this one. If you guys have exam, this should be one of you.  Can you format this kernal function. So you need to think about this should be some phi xi, phi xj here. It means that, and we divide it like 3.    So that you can really notice that this phi xi should be bunch of matrix and phi xj should be another bunch of matrix. Right?   Do you get the point? You should that expand this first. Then write our format in this way. phi x i transposed times with phi x j. you may need a new paper.  Oh, do you want to try it? Did anyone gets some answer? You guys are good. Did anyone get a correct information of it?  Remember, you should write first of matrix. it should be another metrix, so you eventually can get a product as it. You got it? No?   Is it difficult?",
        "week": 11,
        "page": 23
    },
    {
        "transcript": "Okay. And it is a limit time. let's quickly look at it. So let's first  try to expand this. This is this part. Right? First part of this part, and then you have to talk about this.  Just Put them together. Is that correct? You can probably forgot about i and j. Right? So, actually, you have the i equal to one. j is equal to 1, then i is equal to 2. The j is equal to 2.  The reason you have see so many non equation because you can expand that x i here. in this case. And once you try to expand and get the first part of that is your core first part. I mean, this way, I mean, transport.  And eventually, you should have got the product of them. So eventually once you get the transport again, read the summation of it. Right? I see once you see a solution, you see it always kinda easy. Right?  And once you do it, well, it's too difficult. Isn't it? Right? So this part of you will be just positive. Right? So keep in mind that In future, I believe in the future interview, they won't let you do this kind of degradation.  But you probably need to know how to get a kernel. Right? If they ask some question about what is kernel, you probably need to answer that the kernel is just you get a dot product with this one. Right? And we need to remember about those.",
        "week": 11,
        "page": 24
    },
    {
        "transcript": "Okay. And let's continue about some typical bunches of kernels. That's a for some functions to K X I, if it is a kernal function, you definitely need to calculate about this way.  This can be some capstone as you did in this way. It'll be difficult to tell. Right? And then here is a easy way that can help you easily get a to see whether it is a kernel function or not. This should be a Mercer's theorem.  So every semi-positive defined symmetric function is a kernel, which means that semi positive define the definite symmetric functions that correspond to our semi positive define the definite symmetric gram metrix that can be express in this way.  So you'll have a k x y, x x1, x1, k, x1, x2, k, x1, x3, a 2, k, x1, 2, x, and Then you have k x, x2, x1. And blah blah, eventually, you should be what is the matrix size of it?  What is the matrix size of k? it should be n by n metrix. Right? So you need to remember. That is corresponding. You have many samples that you have.",
        "week": 11,
        "page": 25
    },
    {
        "transcript": "And then here are frequently used kernal functions.  The linear one should be just equal to simple one. It's just about matrix . The transpose times with x i. And the polynomial of power, so you should be this 1. And the gaussian kernel, you guys maybe heard about many times. Right?  And this is how we define the kernel the gaussian kernel here, and there is another sigmoid kernal as you can define in this way. So among them, the gaussian kernel actually is Why did you use it in many different applications?  And, also, there is kernel version of the SVM, but that is not a key thing about this lecture. So today's lecture, you guys probably notice how to do SVM and then how the kernel can help you to get a kernel.",
        "week": 11,
        "page": 26
    },
    {
        "transcript": "So, let's try to see about dual problem if we try to get it enrolled with kernels that is nonlinear of SVMs mathematically.  So remember this is about your problem solution. If we think it is a nonlinear problem, so this this part What what are the difference between this one and the previous problem? Can youremember, I quickly identify Where is the difference?  Where is the difference in this one? No difference? And what about solution? Do you see some thing differently in a solution part? Yes. Exactly. So the only difference is about kernel part. Right? You have a k here, k x i x j.  So now, also, in the solution, you have k x i x j. So this should be the difference between the non linear and the linear SVM.   And we also, similarly, try to optimize this the alpha i should be similar as before.",
        "week": 11,
        "page": 27
    },
    {
        "transcript": "And this is about the quick overview of the nonlinear SVM. So SVM locates a separating hyperplane in the feature space, and the classified points in that space that does not need to represent the space explicitly.  Simply by defining kernel function, and the a kernel function place, the role of dot product in the feature space.  So in this case, typically, if we apply the kernel of the SVM, then we are doing another kind of little bit better result So later, you guys can test whether if you apply the kernel of your dataset that can give you a better results or not.",
        "week": 11,
        "page": 28
    },
    {
        "transcript": "And here are some properties of SVM model. So, for example, it is flexible to choose some similarity function and the sparseness of a solution.  when dealing with large data sets, and only support vectors are used to specify the separating hyperplane and the the ability to handle the large feature space.  And the complexity does not depends on the dimensionality of the feature space.  The over fitting problem can be controlled by soft margin approach. Right? And that's nice math property, a simple Convex optimization problem, which is guaranteed to converge to a single global solution.  And, also, you can follow the features selection using some SVM try to find the best feature of it right?",
        "week": 11,
        "page": 29
    },
    {
        "transcript": "And next, we want to quickly talk about some real world applications.  So for example, in the text or have a text classification or the image classification in a bioinformatics, the protein classifications and the cancer classification or hand written character recognition, and most you guys already play with the image classification right?    ''' As I said, in the deep learning course, you guys extract some deep features or your general features. and then apply SVM to get a classification. Right? I don't know whether some of you, get a good results using the deep learning features or not?  Use this on different features or not. to the annualize of your available results. If you use SPM, what is the accuracy where around 65. 65? Oh, that's kinda good. What about others? 64. 64 Right? is better than your own developed cnn model. Right? You would be a little bit sad about that. Yeah. Yeah. Yeah. Yeah. It's common because the key reason is that the resnet 50 because it is already trained on very big data set.  Right? They can extract very good features. So that's can be one reason why your own developed model. It's not as good as the pre trained features.   Right? And then the text just a few minutes, finish the data pipe line, you can get some good results. That's one reason why the combination of SVM can also give kind of good results.  But, you know, it highly relied on your extract features. Right?   because these are features are good. So let's say if your feature is not good, your public cannot cannot do that. '''",
        "week": 11,
        "page": 30
    },
    {
        "transcript": "So, here is about first application in the cancer classification. So in this case, you will have lots of patients like p 1 p 2 to p n, then you have different genes.  Right? Now is more like some capture features. You have g 1 g 2 to a g p here, Let's say if you have very high dimensional space, you have p that is bigger than 1000, and n is less than 100.  Which means is that you have lots of what? Yeah. Lots of what? It's 1,000. It's what? 1000 is what? It's 1000 features features. Right? But they just have few number of patients that is not a lot.  And if, let's say, if it is the imbalance data, you may have a left positive examples. So in this case, if you apply for some kernal function, it will be a kind of good results, and the the might have some irrelevant features.  It's noisy. But in this case, Actually, SVM is sensitive to noise data set. So in this case, you may need to perform a feature selection a little bit  as I said before.  Right? because in the linear case, the w i with power of 2 gives the ranking of dimension of i So in this case, we may have able to determine which is the best features.",
        "week": 11,
        "page": 31
    },
    {
        "transcript": "And, here is just some weakness of SVM model.  As I said, it is very sensitive to the noise, our relative small number of mislabeled examples can dramatically decrease the performance. Why? Why it can  dramatically decrease the performance? if you have some noisy examples.  But as noisy examples mean, noisy example means what? In the wrong group. because these noisy examples can far away from your normal dataset. Right? So remember, in the SVM, we define about hot plates.  Right? This should be some lines. The fitting lines can be affected by those noisy points a lot. Right? Let's say one point far away from other points, you may need to consider that the line will fit this point.  So that's the reason why the performance can be dramatically decreased. So in that case, it means that if you have too many features, its better to think about use feature selection and reduce dimensionality.  so currently it only consider about two classes right?  All the previous slides, we just talk about binary classification.  So how can you do a multi class?  mul class classification problem? How can you do that? select one class and thought others are the rest. Yes. Exactly. I mean, this is called. 1 vs all others. This should be a typical method. So, actually, this is the most thing that you need to do.  So first you need output the arity m with the m SVMs. So, actually, you need to learn lots of SVMs. For example, you should learn svm 1 the output is equal to 1, and it can pair with the output is not equal to 1.  Right? And similarly, you have bunch of other SVM models to detect whether it's true or it is not true, which eventually we are repeatable m types so that you can get m classifier so that we can clarify m different classes.  So that is also one kind of weakness of SVM model. So once you guys implemented it, you guys will have noticed that.  What? But you now have to utilize that? The last step? Yeah. Yes. You need to do that. didn't it didn't all love the reports, just this one fact. Can you repeat the question again?  Then you just have a bunch of leftover points that become there on the left. Mhmm. So you will have m classifiers. Right? For each classifier, it will give you 1 correct class. For m classifier, you will eventaully get the m classes.  I'm saying, if there's m minus one steps, it could just take all points include m.   What where is m minus one steps?   The last one step, it is not necessary.  So you need the last step. Yes. because otherwise, it cannot make a very good good estimation. Right? As as I said, this should be 1 vs all others. You have to make totally m classifiers.   And then you predict the output of new input just predict with each SVM and find out which one puts the prediction the furthest into the positive region.  So that's the reason. to predict m times.",
        "week": 11,
        "page": 32
    },
    {
        "transcript": "And then here's about another view of binary classification compared with multiclass SVM.  So this part is about what we learned before. Right? Have you remembered this one over two w k transposed with w k the result will be bigger or equal to 0, and this is the case that we we have multiple classes right?  And this part should be similar as this one because in this case, you need to minimize all the w transposed with w that in each class. Right? This should be k. And eventually, you have k classes should this time, you should be an object function.  objective function that because this is all classes here, and for your conditions here. There's also similar to the previous one, but in this case, it also needed to consider about h class.  So it should be can have this one. so then eventually, and this is so it has a score for the 2 labels is a pie that's a score for any other label in this case.  So here is a quickly review about difference between this is a binary, and then this should be multi class SVM.",
        "week": 11,
        "page": 33
    },
    {
        "transcript": "And for the second application is that it's about a text categorization. So the task is to actually want to classify different tags or hyper text documents into our fixed number of predefined categories based on their content. For example, like, email, web search, some sort documents by topic.  So our document can be assigned to more than one category so this can be viewed as a series of binary classification problem and a 1 for each category.",
        "week": 11,
        "page": 34
    },
    {
        "transcript": "And here is another way that you can think about how to represent the text.  So Actually, it's i information retrievals vector space are also we have the vector of course representation So our document is representing our back to our index by a pre fixed set or dictionary of terms.  And this is the values of any entry can be binary or weights. We can define as a phi function here as I can get a normalization or stop words word terms, so eventually you try to convert any document x into phi x.  So now the phi x is just about your feature vector. eventually can train on this SVM model use this phi x. Okay?   So nowadays, there are many other ways to convert our text to these features. Like, you can use some BERT, any of you have heard about BERT? kind of language model. Or now even nowadays, you can use some TBT model to convert the text to some features.  And then you guys should have a chance to take that natural language process in the other parts. Right? So, like, I knew you guys will learn more about this part, how to represent the text, get some features.",
        "week": 11,
        "page": 35
    },
    {
        "transcript": "And this is as I said, you probably apply some kernal function here is a distance between 2 different document that is a phi x, come with a phi z, and this is how the kernel function, if it is valid kernal the SVM can be used with the k x Z for the discrimination.  So why should we use SVM? They have all some advantages. For example, it can apply some higher dimension in this space.  And in the if if you have, like, a few irrelevant features, you will have some good dense concepts. And for some sparse documents, you will get some sparse instances, and the text categorization problem, there should be some benefits, just by linear separate, you can get good results. However, if you apply some kernel you will get a little bit higher accuracy.",
        "week": 11,
        "page": 36
    },
    {
        "transcript": "And here, just some quickly issues about the kernels. For example, you may have some problem, choose which kind of kernels. Right? As we already discussed, there are several different kernel functions.  For example, whether I should choose gaussian kernel, or should you choose a polynomial kernel, and if ineffective, more elaborate kernels are needed.  Domain experts can give some assistance in formulating appropriate Similarity measures. You needed to choose some kernel parameters like you need to determine the sigma for the gaussian kernel.  and the sigma is the distance between closest points with different classifications.  And then in the absence of reliable criteria, application is relying on the use of validation set or cross-validation set. So that you can determine the parameters.  And for the optimization criterion, you need to pay addition to the hard margin, vs the soft margin, so a lengthy series of experiments in which various parameters are tested.  You should make sure can get that kind of optimal results in just different data set.",
        "week": 11,
        "page": 37
    },
    {
        "transcript": "And here are some additional sources that if you are interested.  You can take a look at it. For example, you are if you are interested in the VC dimensions, take a look at that. Now here's a more about the SVM, and then even SVR model, what is SVR model?  There's no SVR more. You know, SVR here, but the SVR model is the support vector for regression. So now, today, we just talk about the support vector for classification problem.  Are the support vector can also be applied for regression problem. As most you can take a look at that.",
        "week": 11,
        "page": 38
    },
    {
        "transcript": "And for your guys' homework, you need to implement the SVM model.  But, typically, it should be a multi class SVM model because you will, you know, use the same data set and try to classify them into different classes. So let's quickly look at some of their examples. ",
        "week": 11,
        "page": 40
    }
]
